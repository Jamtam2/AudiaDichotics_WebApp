{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70d13a6-9d0c-4833-9aaf-37d9b889e69d",
   "metadata": {},
   "source": [
    "### <center> The goal of this notebook is to find any urls over the past week that have been accessed within the fedex network that could be harmful or otherwise inappropriate.\n",
    "    \n",
    "- Step <a href='#imports'>#1</a>: Install and Import Libraries\n",
    "- Step <a href='#gcp'>#2</a>: GCP Configurations\n",
    "- Step <a href='#query'>#3</a>: Query Raw URLs\n",
    "- Step <a href='#preprocess'>#4</a>: Preprocessing of the Raw URLs\n",
    "- Step <a href='#early_rdap'>#5</a>: Start RDAP early\n",
    "- Step <a href='#batch_predictions'>#6</a>: Run Batch Prediction on Processed URLs\n",
    "- Step <a href='#user_count'>#7</a>: User Count Scoring\n",
    "- Step <a href='#combine'>#8:</a> Combine Outputs from Jaccard and User Count Scoring\n",
    "- Step <a href='#RDAP'>#9:</a> RDAP Scoring\n",
    "- Step <a href='#total'>#10:</a> Find Total Score\n",
    "- Step <a href='#pipeline'>#11:</a> KubeFlow Pipelining\n",
    "    - Step <a href='#component'>#11.1:</a> Component Initialization\n",
    "    - Step <a href='#define'>#11.2:</a> Defining the pipeline\n",
    "    - Step <a href='#compile'>#11.3:</a> Compile The Pipeline\n",
    "    - Step <a href='#submit'>#11.4:</a> Submitting the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a840df-6f56-4a20-aeea-48a06fc3ac83",
   "metadata": {},
   "source": [
    "## <a id='imports'></a>\n",
    "## <center>Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979aa83-a329-4928-8732-b68e628944bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Install all dependencies from requirements.txt file.\n",
    "\"\"\"\n",
    "       --user: install python packages in the user directory instead of the syste,-wide directory.\n",
    "               Useful if you don't require admin priviledges in your notebook.\n",
    "-q or --quiet: Make command line tools (such as pip) produce less output during execution,\n",
    "               meaning will not show non-essential or verbose information.\n",
    "               Will still show errors or major warnings.\n",
    "\"\"\"\n",
    "%pip install --user --quiet -r requirements.txt\n",
    "print('Imported requirements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2e33a-9922-4fd0-9385-0e209239b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import argparse\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from tld import get_tld\n",
    "\n",
    "import tld\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy\n",
    "\n",
    "import crcmod\n",
    "\n",
    "import termcolor\n",
    "\n",
    "import regex\n",
    "\n",
    "import pydot\n",
    "\n",
    "import objsize\n",
    "\n",
    "import orjson\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "\n",
    "from apache_beam.options.pipeline_options import WorkerOptions\n",
    "\n",
    "from apache_beam.dataframe.convert import to_dataframe\n",
    "\n",
    "from apitools.base import *\n",
    "\n",
    "from apache_beam.io.gcp.internal.clients.storage.storage_v1_client import *\n",
    "\n",
    "from apache_beam.io.gcp.internal.clients.storage.storage_v1_messages import *\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp import dsl\n",
    "\n",
    "import kfp\n",
    "\n",
    "from kfp.components import create_component_from_func\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "import json\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from kfp.v2.dsl import (component, Input, Model, Output, Dataset, \n",
    "                        Artifact, OutputPath, ClassificationMetrics, \n",
    "                        Metrics, InputPath)\n",
    "\n",
    "from google.cloud import aiplatform_v1, bigquery\n",
    "\n",
    "from google_cloud_pipeline_components.v1.dataflow import DataflowPythonJobOp\n",
    "\n",
    "from google_cloud_pipeline_components.v1.wait_gcp_resources import WaitGcpResourcesOp\n",
    "\n",
    "from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde659b-8081-4e55-b8a8-db35bebc8c05",
   "metadata": {},
   "source": [
    "## <a id='gcp'></a>\n",
    "## <center>Step 2: GCP Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d3c4c-7eb6-44d3-82cc-9ea22af2fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/outsidenoxvodafone/vertex-ai-pipeline/blob/main/vertex-ai-pipeline.ipynb\n",
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "### Set up variables\n",
    "PROJECT_ID = 'fxs-gccr-sbd-dev-sandbox' # GCP Project ID\n",
    "REGION = 'us-west1'                     # GCP Region to run pipelines\n",
    "\n",
    "SERVICE_ACCOUNT='my-bigquery-sa@fxs-gccr-sbd-dev-sandbox.iam.gserviceaccount.com' #Service account \n",
    "PIPEINE_ROOT = '/home/jupyter/UMRF-MURLv2/' # location where pipeline's artifacts are stored'\n",
    "BUCKET_NAME = 'suspicious_user_bucket/PipelineOutputs'    # Google Cloud Storage Bucket to store pipeline outputs\n",
    "BIGQUERY_DATASET = 'umrf_malicious_urls'\n",
    "BIGQUERY_TABLE = 'refined_url_features'\n",
    "\n",
    "PIPELINE_NAME = 'test_6-9'\n",
    "JOB_ID = f'Training-pipeline-{TIMESTAMP}'\n",
    "ENABLE_CASHING = False\n",
    "\n",
    "#Just sets it so that it won'we won't have to see DEBUG or INFO messages in our outputs\n",
    "logging.getLogger('fsspec').setLevel(logging.WARNING)\n",
    "\n",
    "# TEMPLATE_PATH = '... .json'\n",
    "\n",
    "# Set the default GCP project\n",
    "# vertex_ai.init(project=PROJECT_ID, location=REGION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f326da-7f75-43a6-a5a2-358268b67046",
   "metadata": {},
   "source": [
    "## <a id='query'></a>\n",
    "# <center> Step 3: Query Raw URLs\n",
    "## This is accomplished by a rudementary query to the cim table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_job() -> None:\n",
    "    import datetime, logging, pandas as pd, sys, time, concurrent.futures, re\n",
    "    from google.cloud import bigquery, storage\n",
    "    import pandas\n",
    "\n",
    "\n",
    "    def dataframe_upload(table, monday, df):\n",
    "        client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "        query = f\"DELETE FROM fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.{table} WHERE week_of = '{monday}'\"\n",
    "        client.query(query)\n",
    "        df.to_gbq(destination_table=f'umrf_murl_v2_results.{table}',\n",
    "                project_id='fxs-gccr-sbd-dev-sandbox', chunksize=1_000_000, if_exists='replace')\n",
    "\n",
    "    def run_query(day, limit):\n",
    "        # BigQuery Query Parameters\n",
    "        my_client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "\n",
    "        whitelist = [\"fedex.com\", \"microsoft.com\", \"akamaihd.net\", \"googleapis.com\", \"amazonaws.com\", \"mcusercontent.com\", \"google.com\", \"oracle.com\", \"facebook.com\", \"amazon.com\", \"bing.com\", \"outlook.com\", \"office.com\", \"live.com\", \"en.wikipedia.org\", \"twitter.com\", \"imdb.com\", \"instagram.com\", \"fandom.com\", \"pinterest.com\", \"nytimes.com\", \"ebay.com\", \"espn.com\", \"etsy.com\", \"apple.com\", \"healthline.com\", \"homedepot.com\", \"mayoclinic.org\", \"cnn.com\", \"linkedin.com\", \"craigslist.org\", \"indeed.com\", \"nih.gov\", \"bestbuy.com\", \"lowes.com\", \"foxnews.com\", \"cdc.gov\", \"cvs.com\", \"irs.gov\", \"britannica.com\", \"clevelandclinic.org\", \"costco.com\", \"medicalnewstoday.com\", \"mail.yahoo.com\", \"forbes.com\", \"ca.gov\", \"cbssports.com\", \"merriam-webster.com\", \"macys.com\", \"allrecipes.com\", \"finance.yahoo.com\", \"nordstrom.com\", \"mapquest.com\", \"cnbc.com\", \"nike.com\", \"businessinsider.com\", \"kohls.com\", \"nypost.com\", \"mlb.com\", \"apartments.com\", \"investopedia.com\", \"expedia.com\", \"chase.com\", \"genius.com\", \"npr.org\", \"foodnetwork.com\", \"nba.com\", \"bankofamerica.com\", \"go.com\", \"accuweather.com\", \"hotels.com\", \"bbc.com\", \"cnet.com\", \"bleacherreport.com\", \"dictionary.com\", \"medlineplus.gov\", \"nbcnews.com\", \"goodhousekeeping.com\", \"basketball-reference.com\", \"kbb.com\", \"att.com\", \"dailymail.co.uk\", \"cbsnews.com\", \"aol.com\", \"hulu.com\", \"ikea.com\", \"listcrawler.eu\", \"newsweek.com\", \"ign.com\", \"ny.gov\", \"dickssportinggoods.com\", \"booking.com\", \"foursquare.com\", \"mcdonalds.com\", \"cosmopolitan.com\", \"nbcsports.com\", \"doordash.com\", \"cars.com\", \"goodreads.com\", \"carfax.com\", \"bloomberg.com\", \"hopkinsmedicine.org\", \"caranddriver.com\", \"adobe.com\", \"insider.com\", \"nerdwallet.com\", \"dominos.com\", \"youtube.com\", \"windowsupdate.com\", \"office.com\"]\n",
    "        whitelist = r\"^.*%s\" % \"$|^.*\".join(map(re.escape, whitelist))\n",
    "        whitelist = whitelist + \"$|^(\\d{{1,3}}\\.){{3}}\\d{{1,3}}$\" \n",
    "\n",
    "        my_query = f\"\"\"\n",
    "            SELECT url, COUNT(DISTINCT user) AS user_count, STRING_AGG(DISTINCT user, ', ') as users\n",
    "            FROM `fxs-entsvcs-eca-ipt-prod-1.fdx_eca_bq_refine_cim_web.refine_cim_web`\n",
    "            WHERE _timestamp BETWEEN \"{day} 00:00:00\" AND \"{day} 23:59:59\"\n",
    "            AND NOT REGEXP_CONTAINS(dest, r\"{whitelist}\")\n",
    "            AND NOT http_method IN ('CONNECT', 'CERTVERIFY')\n",
    "            AND url IS NOT NULL \n",
    "            AND url != 'unknown'\n",
    "            AND action = 'allowed'\n",
    "            GROUP BY url\n",
    "            HAVING user_count < 50\n",
    "            LIMIT {limit}\n",
    "            \"\"\"\n",
    "\n",
    "                    \n",
    "        # BigQuery Test Job Setup\n",
    "        my_config = bigquery.QueryJobConfig()\n",
    "        my_config.dry_run = True\n",
    "        my_test = my_client.query(my_query, job_config=my_config)\n",
    "\n",
    "        gigabytes = my_test.total_bytes_processed/(10**9)\n",
    "        money = (gigabytes/1024)*5\n",
    "        logging.info(f\"This job will take ~{gigabytes:.2f} gigabytes or ~{money:.2f} dollars to run\")\n",
    "\n",
    "        if money > 25:\n",
    "            logging.error(f'query for {day} is too expensive, skipping...')\n",
    "            return None\n",
    "\n",
    "        # Running the Query\n",
    "        raw_predicted_urls = my_client.query(my_query).to_dataframe()\n",
    "        logging.info('did query')\n",
    "\n",
    "        return raw_predicted_urls\n",
    "    \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # Multithreaded Queries\n",
    "    todayDate = datetime.date.today()\n",
    "    most_recent_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=0)\n",
    "    logging.info(f'Most Recent Monday: {str(most_recent_monday)}')\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    logging.info(f'Previous Monday: {str(previous_monday)}')\n",
    "\n",
    "    date_list = [most_recent_monday - datetime.timedelta(days=x+1) for x in range(7)]\n",
    "    urls_w_users = pd.DataFrame(columns=['url', 'user_count'], index=[])\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "\n",
    "        for day in date_list:\n",
    "            logging.info(f\"querying {day}'s urls\")\n",
    "            future = executor.submit(run_query, day, 1_500_000)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is None:\n",
    "                pass\n",
    "            else:\n",
    "                urls_w_users = pd.concat([urls_w_users, result])       \n",
    "\n",
    "    logging.info(\"dropping duplicates.\")\n",
    "    urls_w_users.drop_duplicates(inplace=True)\n",
    "    raw_df = urls_w_users.drop(columns=['user_count','users'])\n",
    "    urls_w_users['users_list'] = urls_w_users['users'].str.split(\", \")\n",
    "    urls_w_users.drop(columns=['users'], inplace=True)\n",
    "\n",
    "    logging.info(\"Sorting dataframes.\")\n",
    "    raw_df.sort_values(by='url', inplace=True)\n",
    "    raw_df.reset_index(drop=True, inplace=True)\n",
    "    raw_df['index'] = raw_df.index\n",
    "    urls_w_users.sort_values(by='url', inplace=True)\n",
    "    urls_w_users.reset_index(drop=True, inplace=True)\n",
    "    urls_w_users['index'] = urls_w_users.index\n",
    "    logging.info(\"dataframes finalized.\")    \n",
    "\n",
    "\n",
    "    raw_df['week_of'] = pd.to_datetime(previous_monday)\n",
    "    urls_w_users['week_of'] = pd.to_datetime(previous_monday)\n",
    "    \n",
    "    dataframe_upload('TEST_query_raw',previous_monday,raw_df )\n",
    "    urls_w_users['users_list'] = urls_w_users[\"users_list\"].apply(lambda x: ','.join(map(str, x)))\n",
    "    #return urls_w_users\n",
    "    dataframe_upload('TEST_exploded_query_w_users', previous_monday, urls_w_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011de078-a0cd-4358-b588-85d7fb2e7022",
   "metadata": {},
   "source": [
    "## <a id='preprocess'></a>\n",
    "# <center> Step 4: Preprocessing of the Raw URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a69610-f08b-463a-96a8-1eaadda6e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def megapreprocess_op() -> None :\n",
    "    import datetime\n",
    "    import concurrent.futures\n",
    "    from google.cloud import bigquery, storage\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import sys\n",
    "    import time\n",
    "    from tld import get_tld\n",
    "    from urllib.parse import urlparse\n",
    "    import tldextract\n",
    "    import string\n",
    "\n",
    "\n",
    "\n",
    "    def dataframe_upload(table, monday, df):\n",
    "        df.to_gbq(destination_table=f'umrf_murl_v2_results.{table}',\n",
    "                project_id='fxs-gccr-sbd-dev-sandbox', chunksize=1_000_000, if_exists='replace')\n",
    "\n",
    " \n",
    "    def megapreprocess(df):\n",
    "    #####\n",
    "    ##### Feature Engineering\n",
    "    #####\n",
    "        def get_domain(url):\n",
    "            url_tld = tldextract.extract(url).suffix\n",
    "            domain_name = tldextract.extract(url).domain\n",
    "            full_domain = domain_name + '.' + url_tld\n",
    "            return full_domain\n",
    "        \n",
    "        \n",
    "        def abnormal_url(url):\n",
    "            hostname = urlparse(url).hostname\n",
    "            if hostname is None:\n",
    "                return 0\n",
    "            if hostname in url:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    " \n",
    "\n",
    "        def get_path(url):\n",
    "            try:\n",
    "                return urlparse(url).path\n",
    "            except Exception as e:\n",
    "                logging.error(f\" EXCEPTION: {e}\")\n",
    "                logging.error(f\"getting path function failed!\\tURL: {url}\")\n",
    "\n",
    "\n",
    "        def letter_count(url):\n",
    "            try:\n",
    "                letters = 0\n",
    "                for i in url:\n",
    "                    if i.isalpha():\n",
    "                        letters += 1\n",
    "                return letters\n",
    "            except Exception as e:\n",
    "                logging.error(f\" EXCEPTION: {e}\")\n",
    "                logging.error(f\"Counting letters failed!\\tURL: {url}\")\n",
    "\n",
    "        def count_subs(url):\n",
    "            sub = tldextract.extract(url).subdomain\n",
    "            if sub == \"\":  # if .split is run on the empty string it makes a list of length 1 with an empty element instead of an empty list\n",
    "                return 0\n",
    "            else:\n",
    "                return len(sub.split(\".\"))\n",
    "        def get_port(url):\n",
    "                    try:\n",
    "                        return urlparse(url).port\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\" EXCEPTION: {e}\")\n",
    "                        logging.error(f\"getting port function failed!\\tURL: {url}\")\n",
    "\n",
    "        def port_sus(port):\n",
    "                    if port is None:\n",
    "                        return 0.5\n",
    "                    elif port == 80 or port == 443:\n",
    "                        return 1   \n",
    "                    if 0 <= port <= 1023:\n",
    "                        return 0.5\n",
    "                    else: \n",
    "                        return 0\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        try:\n",
    "            logging.info(\"in the thread function\")\n",
    "            df['domain'] = df['url'].apply(lambda i: get_domain(i))\n",
    "            logging.info(\"Got domains\")\n",
    "            df['path'] = df['url'].apply(lambda i: get_path(i))\n",
    "            logging.info(\"Got paths\")\n",
    "            df['tld'] = df['url'].apply(lambda i: get_tld(i,fail_silently=True)).fillna('')\n",
    "            logging.info(\"Got TLDs\")\n",
    "\n",
    "            last = time.time()\n",
    "            ipv4 = r'(?:^(?:(?:(?:25[0-5])|(?:2[0-4]\\d)|(?:\\d|1\\d{2}))\\.){3}(?:(?:25[0-5])|(?:2[0-4]\\d)|(?:\\d|1\\d{2})))'\n",
    "            ipv6 = r'(?:^(?:[0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4})'\n",
    "            any_ip = f\"{ipv4}|{ipv6}\"\n",
    "            df['having_ip_address'] = df['url'].str.contains(any_ip)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: ip address\")\n",
    "            logging.info(\"1/33\")\n",
    "            last = current\n",
    "            \n",
    "            # TODO: Can be improved upon by looking at multiple characterics, and the greater the number of abnormalities in different checks, we can normalize that score for the model.\n",
    "            # THis currenty returns 1 for a lot of URLs.\n",
    "            df['abnormal_url'] = df['url'].apply(abnormal_url)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: abnormal\")\n",
    "            logging.info(\"2/33\")\n",
    "            last = current\n",
    "            \n",
    "            df['count_dot'] = df['url'].str.count(r'\\.')\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: .\")\n",
    "            logging.info(\"3/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_www'] = df['url'].str.count('www')\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: www\")\n",
    "            logging.info(\"4/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_atrate'] = df['url'].str.count('@')\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: @\")\n",
    "            logging.info(\"5/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_https'] = df['url'].str.count(\"https\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: https\")\n",
    "            logging.info(\"6/33\")\n",
    "            last = current\n",
    "\n",
    "            shorteners = [\"bit.ly\", \"goo.gl\", \"shorte.st\", \"go2l.ink\", \"x.co\", \"ow.ly\", \"t.co\", \"tinyurl\",\n",
    "                \"tr.im\", \"is.gd\", \"cli.gs\", \"yfrog.com\", \"migre.me\", \"ff.im\", \"tiny.cc\", \"url4.eu\",\n",
    "                \"twit.ac\", \"su.pr\", \"twurl.nl\", \"snipurl.com\", \"short.to\", \"BudURL.com\", \"ping.fm\",\n",
    "                \"post.ly\", \"Just.as\", \"bkite.com\", \"snipr.com\", \"fic.kr\", \"loopt.us\", \"doiop.com\",\n",
    "                \"short.ie\", \"kl.am\", \"wp.me\", \"rubyurl.com\", \"om.ly\", \"to.ly\", \"bit.do\", \"lnkd.in\",\n",
    "                \"db.tt\", \"qr.ae\", \"adf.ly\", \"bitly.com\", \"cur.lv\", \"tinyurl.com\", \"ity.im\", \"q.gs\",\n",
    "                \"po.st\", \"bc.vc\", \"twitthis.com\", \"u.to\", \"j.mp\", \"buzurl.com\", \"cutt.us\", \"u.bb\",\n",
    "                \"yourls.org\", \"prettylinkpro.com\", \"scrnch.me\", \"filoops.info\", \"vzturl.com\", \"qr.net\",\n",
    "                \"1url.com\", \"tweez.me\", \"v.gd\", \"link.zip.net\"]\n",
    "            pattern = r\"^(%s)\" % \"|\".join(map(re.escape, shorteners))\n",
    "            df['short_url'] = df['domain'].str.contains(pattern)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: shortening_service list\")\n",
    "            logging.info(\"7/33\")\n",
    "            last = current\n",
    "\n",
    "            df['count_http'] = df['url'].str.count(\"http\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: http\")\n",
    "            logging.info(\"8/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_ques'] = df['url'].str.count(r\"\\?\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: ?\")\n",
    "            logging.info(\"9/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_per'] = df['url'].str.count(r\"\\%\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: %\")\n",
    "            logging.info(\"10/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_hyphen'] = df['url'].str.count(\"-\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: -\")\n",
    "            logging.info(\"11/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_equal'] = df['url'].str.count(\"=\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: =\")\n",
    "            logging.info(\"12/33\")\n",
    "            last = current\n",
    " \n",
    "            df['url_length'] = df['url'].str.len()\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: url length\")\n",
    "            logging.info(\"13/33\")\n",
    "            last = current\n",
    " \n",
    "            df['hostname_length'] = df['domain'].str.len()\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: domain length\")\n",
    "            logging.info(\"14/33\")\n",
    "            last = current\n",
    " \n",
    " \n",
    "            suspicious_words = [\"PayPal\", \"login\", \"signin\", \"bank\", \"account\", \"update\", \"free\", \"lucky\", \"service\", \"bonus\", \"ebayisapi\", \"webscr\"]\n",
    "            df['sus_url'] = df['url'].apply(lambda url: any(word in url for word in suspicious_words))\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: sus url with list\")\n",
    "            logging.info(\"15/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_digits']= df['url'].str.count(r\"\\d\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: digit count\")\n",
    "            logging.info(\"16/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_letters']= df['url'].apply(lambda i: letter_count(i))\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: letter lambda\")\n",
    "            logging.info(\"17/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_dir'] = df['path'].str.count('/')\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: /\")\n",
    "            logging.info(\"18/33\")\n",
    "            last = current\n",
    " \n",
    "            df['count_embed_domain'] = df['path'].str.count(\"//\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: embed\")\n",
    "            logging.info(\"19/33\")\n",
    "            last = current\n",
    " \n",
    "            current = time.time()\n",
    "            df['fd_length'] = df['path'].str.split('/').str[1].str.len().fillna(0)\n",
    "            logging.info(f\"{current-last} function run: fd length\")\n",
    "            logging.info(\"20/33\")\n",
    "            last = current\n",
    " \n",
    "            df['tld_length'] = df['tld'].str.len()\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: tld length\")\n",
    "            logging.info(\"21/33\")\n",
    "            last = current\n",
    "\n",
    "            df['double_slash'] = df['url'].str.count(\"//\")\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: double slash\")\n",
    "            logging.info(\"22/33\")\n",
    "            last = current\n",
    "\n",
    "            df['subdomain_count'] = df['url'].apply(count_subs)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: subdomain counter\")\n",
    "            logging.info(\"23/33\")\n",
    "            last = current\n",
    "\n",
    "            pattern = '[' + '!@#$%^&*()_+,-./:;<=>?@[\\]^_`{|}~' + ']'\n",
    "            df['special_chars'] = df['url'].str.count(pattern)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: special characters\")\n",
    "            logging.info(\"24/33\")\n",
    "            last = current\n",
    "\n",
    "            df['ratio_digits'] = df['count_digits'] / df['url_length']\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: ratio digits\")\n",
    "            logging.info(\"25/33\")\n",
    "            last = current\n",
    "\n",
    "\n",
    "            suspicious_keywords = [ \n",
    "            'PayPal', 'login', 'signin', 'bank', 'account', 'update', 'free', 'lucky', 'service', 'bonus', 'ebayisapi', 'webscr', 'login', 'signin', \n",
    "            'password', 'secure', 'account', 'verification', 'validate', 'confirm', 'token', 'update', 'registry', 'payment', 'credit', 'transaction', 'admin', 'service', \n",
    "            'webmaster', 'helpdesk', 'paypal', 'ebay', 'amazon', 'bank', 'wellsfargo', 'chase', 'citi', 'boa', 'fedex', 'microsoft', '.exe', '.zip', '.rar', '.doc', '.xls', '.pdf', 'free', \n",
    "            'gift', 'promo', 'offer', 'download', '.dll', 'prize', 'reward', 'sweepstakes', 'lottery', 'winner', 'congratulations', 'script', 'stream', 'play', 'game', 'invoke', 'download', \n",
    "            'cdn', 'media', 'video', 'manga']\n",
    "\n",
    "            df['sus_words'] = df['url'].apply(lambda url: any(word in url for word in suspicious_keywords))\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: sus url with list\")\n",
    "            logging.info(\"26/33\")\n",
    "            last = current\n",
    "\n",
    "            risky_extensions = ['.exe', '.swf', '.php']\n",
    "            pattern = r\"(%s)\" % \"|\".join(map(re.escape, risky_extensions))\n",
    "            df['risky_ext'] = df['url'].str.contains(pattern)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: risky extensions\")\n",
    "            logging. info(\"27/33\")\n",
    "            last = current\n",
    "\n",
    "            suspicious_TLDs = [\n",
    "            \"top\", \"tk\", \"xyz\", \"tw\", \"loan\", \"ga\", \"ml\", \"cf\", \"gq\", \"club\", \"online\",\n",
    "            \"site\", \"ltd\", \"work\", \"vip\", \"icu\", \"pw\", \"cc\", \"gdn\", \"men\", \"win\",\n",
    "            \"space\", \"fun\", \"stream\", \"bid\", \"review\", \"trade\", \"host\", \"cloud\", \"date\",\n",
    "            \"download\", \"party\", \"ink\", \"science\", \"racing\", \"xin\", \"accountant\", \"faith\",\n",
    "            \"webcam\", \"xn--fiqs8s\", \"xn--fiqz9s\", \"xn--p1acf\", \"xn--6qq986b3xl\", \"cricket\",\n",
    "            \"tel\", \"xn--55qx5d\", \"page\", \"support\", \"co.zw\", \"xn--io0a7i\", \"gold\",\n",
    "            \"xn--80adxhks\", \"casa\", \"xn--czru2d\", \"dev\", \"business\", \"xn--3ds443g\",\n",
    "            \"gallery\", \"in.net\", \"zw\", \"bd\", \"ke\", \"am\", \"sbs\", \"quest\", \"cd\", \"cyou\",\n",
    "            \"rest\", \"help\", \"ws\", \"tokyo\", \"cam\", \"cm\", \"uno\", \"email\", \"info\", \"su\",\n",
    "            \"best\", \"ms\", \"country\", \"jetzt\", \"kim\", \"mom\", \"wang\", \"ninja\", \"zip\",\n",
    "            \"realtor\", \"christmas\", \"pro\", \"sx\", \"link\", \"biz\", \"yokohama\", \"ooo\",\n",
    "            \"ryukyu\", \"mw\", \"ci\", \"bar\", \"surf\", \"cn\", \"fit\"\n",
    "            ]\n",
    "            pattern = r\"^(%s)\" % \"|\".join(map(re.escape, suspicious_TLDs))\n",
    "            df['sus_tld'] = df['tld'].str.contains(pattern)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: suspicious TLDs\")\n",
    "            logging. info(\"28/33\")\n",
    "            last = current\n",
    "\n",
    "            allowed_chars = string.ascii_letters + string.digits + string.punctuation\n",
    "            translation_table = str.maketrans(\"\", \"\", allowed_chars)\n",
    "            df['nonstandard_chars'] = df['url'].str.translate(translation_table) != \"\"\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: non-standard characters\")\n",
    "            logging.info(\"29/33\")\n",
    "            last = current\n",
    "\n",
    "            df['port'] = df['url'].apply(get_port)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: getting port\")\n",
    "            logging.info(\"30/33\")\n",
    "            last = current\n",
    "\n",
    "            df['has_port'] = ~df['port'].isna()\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: has port binary\")\n",
    "            logging.info(\"31/33\")\n",
    "            last = current\n",
    "\n",
    "            df['sus_port'] = df['port'].apply(port_sus)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: suspicious ports\")\n",
    "            logging.info(\"32/33\")\n",
    "            last = current\n",
    "\n",
    "\n",
    "\n",
    "            suspicious_patterns = ['DROP', 'INSERT', 'DELETE']  # expand as needed\n",
    "            pattern = r\"(%s)\" % \"|\".join(map(re.escape, suspicious_patterns))\n",
    "            df['sus_query'] = df['url'].str.contains(pattern)\n",
    "            current = time.time()\n",
    "            logging.info(f\"{current-last} function run: malformed structures\")\n",
    "            logging.info(\"33/33\")\n",
    "            last = current\n",
    "\n",
    "            adult_sus_words = [\n",
    "            'camgirl', 'porn', 'xxx',  'pornhub', 'xvideo', 'xhamster', 'nsfw', 'only','feet', 'kink', 'nude', 'leaked', 'boob', 'boobs', 'lesbian', 'blacked', \n",
    "            'leaks', 'fap', 'voyuer'\n",
    "            ]\n",
    "            pattern = r\"(%s)\" % \"|\".join(map(re.escape, adult_sus_words))\n",
    "            df['adult_words'] = df['url'].str.contains(pattern)\n",
    "\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            re_add_columns = df[['url', 'index', 'path', 'domain','week_of','tld']] #these are the columns we need to add back in after normalization\n",
    "            df = df.drop(columns=['url', 'tld', 'index', 'path', 'domain', 'week_of'])  # these are the columns we remove\n",
    "            # tld does not get added back in because its only needed during preprocessing\n",
    "            logging.info('normalizing')\n",
    "            df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "            logging.info('normalizing done. Readding columns.')\n",
    "            re_add_columns = re_add_columns.reset_index(drop=True)\n",
    "            df.index = re_add_columns.index\n",
    "            df = pd.concat([df, re_add_columns], axis=1)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\" EXCEPTION: {e}\")\n",
    "            logging.error(\"preprocessing function errored. adding df to list of failures.\")\n",
    "            failed_dfs.append(df)\n",
    "            return None\n",
    "\n",
    " \n",
    "    #start of component\n",
    "    start = time.time()\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "    client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "    todayDate = datetime.date.today()\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    print(previous_monday)\n",
    "    query = f'SELECT * FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_query_raw` WHERE week_of = \"{previous_monday}\"' \n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    #TESTING CODE,IGNORE, BUT KEEP IN FOR EASE OF ACCESS FOR TESTING\n",
    "    # df = pd.read_csv('gs://suspicious_user_bucket/data/misc_csvs/raw_df_tester.csv')  \n",
    "    # df.drop(columns=['user_count'],inplace=True)\n",
    "    # df['index'] = df.index\n",
    "    # df['week_of'] = pd.to_datetime(previous_monday)\n",
    "\n",
    "\n",
    "    logging.info(f\"got {len(df)} rows\")\n",
    "    per_df_rows = 750_000\n",
    "    count_of_dfs = len(df) // per_df_rows\n",
    "    list_of_dfs = []\n",
    "    if len(df) % per_df_rows != 0:\n",
    "        count_of_dfs += 1 # 3.5m gives 4, 3m gives 3\n",
    "    for i in range(count_of_dfs):\n",
    "        start_index = i * per_df_rows\n",
    "        end_index = start_index + per_df_rows\n",
    "        list_of_dfs.append(df.iloc[start_index: end_index])\n",
    "    \n",
    "    \n",
    "    processed_dfs = []\n",
    "    failed_dfs = []\n",
    "    logging.info(\"its threading time\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Mapping index and url to process_url function\n",
    "        futures = []\n",
    "        for df in list_of_dfs:\n",
    "            future = executor.submit(megapreprocess, df)\n",
    "            logging.info(\"submitted df\")\n",
    "            futures.append(future)\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            res = future.result()\n",
    "            if res is not None:\n",
    "                processed_dfs.append(future.result())\n",
    "    logging.info(\"All Dataframes processed.\")\n",
    "    logging.info(f\"{len(processed_dfs)} dfs finished successfully\")\n",
    "    if len(processed_dfs) == len(list_of_dfs):  # all dataframes succeeded\n",
    "        logging.info(\"All dataframes succeeded\")\n",
    "        df = pd.concat(processed_dfs)\n",
    "    else:\n",
    "        logging.error(\"Some dataframes failed. Exiting...\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    df = df.astype('str')    \n",
    "\n",
    "    phish_cols = [\"abnormal_url\",    \"count_atrate\",    \"count_digits\",    \"count_dir\",    \"count_dot\",    \"count_embed_domain\",    \"count_equal\",    \"count_http\",    \"count_https\",    \"count_hyphen\",    \"count_letters\",    \"count_per\",    \"count_ques\",    \"count_www\",    \"fd_length\",    \"having_ip_address\",    \"hostname_length\",    \"index\",   \"short_url\",    \"sus_url\",    \"sus_words\",    \"tld_length\",    \"url\",    \"url_length\"]\n",
    "    refined_cols =    [\"abnormal_url\",    \"adult_words\",    \"count_atrate\",    \"count_digits\",    \"count_dir\",    \"count_dot\",    \"count_embed_domain\",    \"count_equal\",    \"count_https\",    \"count_hyphen\",    \"count_letters\",    \"count_per\",    \"count_ques\",    \"count_www\",    \"domain\",    \"double_slash\",    \"fd_length\",    \"hostname_length\",    \"index\",    \"nonstandard_chars\",    \"path\",    \"ratio_digits\",    \"risky_ext\",    \"special_chars\",    \"subdomain_count\",    \"sus_port\",    \"sus_query\",    \"sus_tld\",    \"sus_words\",    \"tld\",     \"url\",    \"url_length\"]\n",
    "\n",
    "    refined_df = df[refined_cols]\n",
    "    phish_df = df[phish_cols]\n",
    "\n",
    "    dataframe_upload('refined_model_bq_input', previous_monday, refined_df)\n",
    "    dataframe_upload('phish_model_bq_input', previous_monday, phish_df)\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    timer = f\"total time to run: {end-start}\"\n",
    "    logging.info(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#megapreprocess_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc283a",
   "metadata": {},
   "source": [
    "## <a id='early_rdap'></a>\n",
    "# <center> Step 5: Start RDAP early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea69766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_RDAP() -> None:\n",
    "    import ast\n",
    "    import concurrent.futures\n",
    "    import datetime\n",
    "    from google.cloud import aiplatform_v1 as aiplatform, bigquery, storage\n",
    "    import logging\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import threading\n",
    "    import time\n",
    "    from urllib.parse import urlparse\n",
    "    import tldextract\n",
    "\n",
    "\n",
    "    def  batch_pred_checker():\n",
    "            def check_batch_preds(response):\n",
    "                i = 0\n",
    "                checker = 0\n",
    "\n",
    "                for job in response:\n",
    "                    print(f'Job status: {job.state.name}')\n",
    "                    if i == 2:\n",
    "                        logging.info('not done yet, waiting 1 minute')\n",
    "                        return False\n",
    "                    if job.state.name == 'JOB_STATE_RUNNING':\n",
    "                        checker = checker + 1\n",
    "                    if checker == 2:\n",
    "                        return True\n",
    "                    i = i+1\n",
    "\n",
    "\n",
    "            client = aiplatform.JobServiceClient(client_options={\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"})\n",
    "            parent = f\"projects/{project_id}/locations/{location}\"\n",
    "            \n",
    "            #checking if Batch Predictions have  started, if they have, move on to checking if they have ended\n",
    "            batch_preds_started = False\n",
    "            while (not batch_preds_started) and not stop_event.is_set():\n",
    "                response = client.list_batch_prediction_jobs(parent=parent)\n",
    "                \n",
    "                if check_batch_preds(response):\n",
    "                    batch_preds_started = True\n",
    "                else:\n",
    "                    end = time.time()\n",
    "                    if (end - start) >= 3600:\n",
    "                        print(f\"batch preds are bugged, or taking longer than an hour, ending program\")\n",
    "                        stop_event.set()\n",
    "                        break\n",
    "\n",
    "                    time.sleep(60)\n",
    "\n",
    "\n",
    "            #Just check if it's in any state that isn't Running, if neither batch preds is running, stop.\n",
    "            batch_preds_ended = False\n",
    "            while(not batch_preds_ended) and not stop_event.is_set():\n",
    "                response = client.list_batch_prediction_jobs(parent=parent)\n",
    "                \n",
    "                if not check_batch_preds(response):\n",
    "                    batch_preds_ended = True\n",
    "                    stop_event.set()\n",
    "                else:\n",
    "                    end = time.time()\n",
    "                    if (end - start) >= 3600:\n",
    "                        print(f\"batch preds are bugged, or taking longer than an hour, ending program\")\n",
    "                        stop_event.set()\n",
    "                        break\n",
    "\n",
    "                    time.sleep(60)\n",
    "\n",
    "            stop_event.set()\n",
    "            #Return true if both have stopped\n",
    "            return True  \n",
    "\n",
    "        \n",
    "    def read_dict_from_gcs(blob):\n",
    "        # Download the file as a string\n",
    "        file_content = blob.download_as_text()\n",
    "\n",
    "        # Parse the string into a dictionary using ast\n",
    "        dictionary = ast.literal_eval(file_content)\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    def upload_dict_to_gcs(dictionary, bucket_name, destination_file_name):\n",
    "        # Convert the dictionary to a string\n",
    "        dictionary_string = str(dictionary)\n",
    "\n",
    "        # Instantiates a client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Retrieves the bucket\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "        # Uploads the dictionary string to GCS as a .txt file\n",
    "        blob = bucket.blob(destination_file_name)\n",
    "        blob.upload_from_string(dictionary_string, content_type='text/plain')\n",
    "\n",
    "        print(f\"Dictionary uploaded to GCS bucket {bucket_name} as {destination_file_name}.\")\n",
    "\n",
    "\n",
    "    def get_basic_rdap(attempts):\n",
    "        rdap_bootstrap_url = \"https://data.iana.org/rdap/dns.json\"\n",
    "        \n",
    "        # TODO: No proxy's are being exported, so no proxy is ever used.\n",
    "        # Get proxy settings from environment variables (if there are any)\n",
    "        proxies = {\n",
    "            'http': os.environ.get('http_proxy'),\n",
    "            'https': os.environ.get('https_proxy')\n",
    "        }\n",
    "        for i in range(attempts):\n",
    "            # Fetch RDAP bootstrap data using proxy settings and 10 seconds timeout\n",
    "            try:\n",
    "                bootstrap_data = requests.get(rdap_bootstrap_url, proxies=proxies, timeout=10).json()\n",
    "                logging.info(f'RDAP Successful {i}')\n",
    "                return bootstrap_data\n",
    "            except requests.exceptions.Timeout:\n",
    "                logging.warning(\"Timeout error fetching RDAP bootstrap data.\")\n",
    "            except requests.exceptions.TooManyRedirects:\n",
    "                logging.warning(\"Too many redirects when fetching RDAP bootstrap data.\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.warning(f\"Error fetching RDAP bootstrap data: {e}\")\n",
    "            except ValueError:\n",
    "                logging.warning(\"Invalid JSON received from RDAP bootstrap service.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def get_rdap_url(domain, bootstrap_data):\n",
    "        # Iterate through each entry in the 'services' list in the JSON response data\n",
    "        for entry in bootstrap_data['services']:\n",
    "            # Iterate through each domain pattern in the first element of the entry\n",
    "            for domain_pattern in entry[0]:\n",
    "                # If 'domain' ends with the domain pattern\n",
    "                if domain.endswith(domain_pattern):\n",
    "                    # Return the first RDAP server listed in the second element of the entry\n",
    "                    # Can be queried for info about the domain\n",
    "                    return entry[1][0] # return the first RDAP server URL for this TLD\n",
    "        # Return None if no matching RDAP server is found\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def domain_getter(url):\n",
    "        url_tld = tldextract.extract(url).suffix\n",
    "        domain_name = tldextract.extract(url).domain\n",
    "        full_domain = domain_name + '.' + url_tld\n",
    "        return full_domain\n",
    "    \n",
    "\n",
    "    def domain_func(domain, bootstrap_data):\n",
    "        nonlocal already_cached\n",
    "        nonlocal failures\n",
    "        #starting thread\n",
    "        while not stop_event.is_set():\n",
    "                    \n",
    "            if domain in domain_cache:\n",
    "                print('how tf u in here bruh')\n",
    "                return \n",
    "            \n",
    "            # If the domain starts with 'www.', remove the prefix\n",
    "            if domain.startswith('www.'):\n",
    "                domain = domain[4:]\n",
    "            # Get the RDAP server URL for the domain\n",
    "            rdap_url = get_rdap_url(domain, bootstrap_data)\n",
    "            \n",
    "            if not rdap_url:\n",
    "                return \n",
    "            # Construct the RDAP info URL\n",
    "            if rdap_url[-1] == \"/\":\n",
    "                rdap_info_url = f\"{rdap_url}domain/{domain}\"\n",
    "            else:\n",
    "                rdap_info_url = f\"{rdap_url}/domain/{domain}\"\n",
    "            try:\n",
    "                # Send a GET request to the RDAP info URL with a 10 seconds timeout\n",
    "                with rdap_semaphore:\n",
    "                    nonlocal request_counter\n",
    "                    request_counter += 1\n",
    "                    if request_counter % 1000 == 0:\n",
    "                        logging.info(f\"{request_counter} done {time.time()}\")\n",
    "                    response = requests.get(rdap_info_url, timeout=10)\n",
    "                # If the request is successful, return the JSON response data\n",
    "                if response.status_code == 200:\n",
    "                    response = response.json()\n",
    "                else:\n",
    "                    logging.error(f\"Error fetching RDAP info for '{domain}': HTTP {response.status_code}. Reason: {response.reason}\")\n",
    "                    failures += 1\n",
    "                    logging.info(f'Failures: {failures}')\n",
    "                    return \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Error fetching RDAP info for '{domain}': {e}\")\n",
    "                return \n",
    "            \n",
    "            \n",
    "            if response is None or 'events' not in response:\n",
    "                return \n",
    "            creation_date = None\n",
    "            for event in response['events']:\n",
    "                if event['eventAction'] == 'registration':\n",
    "                    creation_date = datetime.strptime(event['eventDate'][0:10], '%Y-%m-%d')\n",
    "                    domain_cache[domain] = creation_date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Start of Component\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Starting\")\n",
    "    start = time.time()\n",
    "    stop_event = threading.Event()\n",
    "    failures = 0\n",
    "    already_cached = 0\n",
    "    request_counter = 0\n",
    "    project_id = \"fxs-gccr-sbd-dev-sandbox\"\n",
    "    location = \"us-west1\"\n",
    "    t1 = threading.Thread(target=batch_pred_checker)\n",
    "    t1.start()\n",
    "    logging.info(\"batch checker started\")\n",
    "    \n",
    "    \n",
    "    cache_bucket = \"suspicious_user_bucket\"\n",
    "    cache_file_name = \"rdap_cache/rdap_domains.txt\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(cache_bucket)\n",
    "    blob = bucket.blob(cache_file_name)\n",
    "    domain_cache = {}\n",
    "    if blob.exists():\n",
    "        domain_cache = read_dict_from_gcs(blob)\n",
    "    logging.info(\"got cache\")\n",
    "    domains = {}\n",
    "    todayDate = datetime.date.today()\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "    query = f'SELECT * FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_query_raw` WHERE week_of = \"{previous_monday}\"'\n",
    "    df = client.query(query).to_dataframe()\n",
    "    df_len = len(df)\n",
    "    logging.info(\"getting domains\")\n",
    "    df['domain'] = df['url'].apply(domain_getter)\n",
    "    logging.info(\"got domains\")\n",
    "    logging.info(len(domains))\n",
    "    domains = set(df['domain'])\n",
    "    domains = domains - set(domain_cache.keys())\n",
    "    bootstrap_data = get_basic_rdap(attempts=10)\n",
    "    if bootstrap_data is None:\n",
    "        logging.critical(\"PLEASE HELP RDAP FAILED 10 TIMES IN A ROW THIS IS REALLY REALLY BAD\")\n",
    "        return\n",
    "    rdap_semaphore = threading.Semaphore(4)\n",
    "    logging.info(\"Starting threads\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for domain in domains:\n",
    "            if stop_event.is_set():  # If batch predictions are done, stop creating tasks\n",
    "                break\n",
    "            executor.submit(domain_func, domain, bootstrap_data)\n",
    "\n",
    "    # Call the function to upload the dictionary to GCS\n",
    "    upload_dict_to_gcs(domain_cache, cache_bucket, cache_file_name)\n",
    "    stop_event.set()\n",
    "    end = time.time()\n",
    "    timer = f\"total time to run: {end-start}\"\n",
    "    logging.info(timer)\n",
    "    logging.info(f\"Requests sent: {request_counter}\")\n",
    "    logging.info(f'Failures: {failures}')\n",
    "    logging.info(f'Already cached: {already_cached}')\n",
    "    logging.info(f'DF length: {df_len}')\n",
    "    logging.info(f'Failure rate: {failures/df_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_RDAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa653b-2e2f-4f6c-a1da-4665c557ee9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a id='batch_predictions'></a>\n",
    "# <center> Step #6: Run Batch Prediction on Processed URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f1fbb-fa66-4855-93d4-6078975a984b",
   "metadata": {},
   "source": [
    "The cell below does a lot\n",
    "\n",
    "here's a breakdown\n",
    "\n",
    "first we get the models for both phishpoint and murl\n",
    "\n",
    "next we set up batch jobs to run for both (this is where the predictions of what is bad or good is done)\n",
    "\n",
    "then we get table ids from the output of the batch jobs (this is where the data got sent after the predictions)\n",
    "\n",
    "then we take the score from one of the data frames and add it to the dataframe made by the other score table\n",
    "\n",
    "that data frame with both scores is the end result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c32599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_refined = ['special_chars','nonstandard_chars','risky_ext','ratio_digits','double_slash','sus_query','sus_tld','adult_words','sus_port','subdomain_count']\n",
    "\n",
    "\n",
    "not_in_phish = ['sus_words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f4d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query =f\"\"\"\n",
    "                        -- Create the phish table using the DistinctURLs temporary table\n",
    "                        CREATE OR REPLACE TABLE `fxs-gccr-sbd-dev-sandbox.PHISH_FEATURE_ATTR.current_feature_attribute_phish` AS\n",
    "SELECT d.url, \n",
    "       p.url AS old_url,\n",
    "       p.having_ip_address AS having_ip_address,\n",
    "       p.abnormal_url AS abnormal_url,\n",
    "       p.count_dot AS count_dot,\n",
    "       p.count_www AS count_www,\n",
    "       p.count_atrate AS count_atrate,\n",
    "       p.count_https AS count_https,\n",
    "       p.short_url AS short_url,\n",
    "       p.count_http AS count_http,\n",
    "       p.count_ques AS count_ques,\n",
    "       p.count_per AS count_per,\n",
    "       p.count_hyphen AS count_hyphen,\n",
    "       p.count_equal AS count_equal,\n",
    "       p.url_length AS url_length,\n",
    "       p.hostname_length AS hostname_length,\n",
    "       p.sus_url AS sus_url,\n",
    "       p.count_digits AS count_digits,\n",
    "       p.count_letters AS count_letters,\n",
    "       p.count_dir AS count_dir,\n",
    "       p.count_embed_domain AS count_embed_domain,\n",
    "       p.fd_length AS fd_length,\n",
    "       p.tld_length AS tld_length,\n",
    "       p.index AS index\n",
    "\n",
    "                            FROM DistinctURLs d \n",
    "                            JOIN `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.phish_model_bq_input` p\n",
    "                            ON d.url = p.url;\n",
    "\n",
    "                        -- Create the refined table using the DistinctURLs temporary table\n",
    "                        CREATE OR REPLACE TABLE `fxs-gccr-sbd-dev-sandbox.REFINED_FEATURE_ATTR.current_feature_attribute_refined` AS\n",
    "SELECT d.url, \n",
    "       p.abnormal_url AS abnormal_url,\n",
    "       p.count_dot AS count_dot,\n",
    "       p.count_www AS count_www,\n",
    "       p.count_atrate AS count_atrate,\n",
    "       p.count_https AS count_https,\n",
    "       p.count_ques AS count_ques,\n",
    "       p.count_per AS count_per,\n",
    "       p.count_hyphen AS count_hyphen,\n",
    "       p.count_equal AS count_equal,\n",
    "       p.url_length AS url_length,\n",
    "       p.hostname_length AS hostname_length,\n",
    "       p.sus_words AS sus_words,\n",
    "       p.count_digits AS count_digits,\n",
    "       p.count_letters AS count_letters,\n",
    "       p.count_dir AS count_dir,\n",
    "       p.count_embed_domain AS count_embed_domain,\n",
    "       p.fd_length AS fd_length,\n",
    "       p.special_chars AS special_chars,\n",
    "       p.nonstandard_chars AS nonstandard_chars,\n",
    "       p.risky_ext AS risky_ext,\n",
    "       p.ratio_digits AS ratio_digits,\n",
    "       p.double_slash AS double_slash,\n",
    "       p.sus_query AS sus_query,\n",
    "       p.sus_tld AS sus_tld,\n",
    "       p.adult_words AS adult_words,\n",
    "       p.sus_port AS sus_port,\n",
    "       p.ip_address AS ip_address,\n",
    "       p.subdomain_count AS subdomain_count,\n",
    "       p.url AS old_url,\n",
    "       p.index AS index,\n",
    "       p.domain AS domain,\n",
    "       p.path AS path,\n",
    "       p.tld AS tld\n",
    "                            FROM DistinctURLs d \n",
    "                            JOIN `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.refined_model_bq_input` p\n",
    "                            ON d.url = p.url;\n",
    "\n",
    "                        -- Drop the temporary table\n",
    "                        DROP TABLE DistinctURLs;\n",
    "                        END;\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = {\"bq://fxs-gccr-sbd-dev-sandbox.PHISH_FEATURE_ATTR\":'phishpoint_model', \"bq://fxs-gccr-sbd-dev-sandbox.REFINED_FEATURE_ATTR\":'refined_model'}\n",
    "bq_input_uris = ['bq://fxs-gccr-sbd-dev-sandbox.REFINED_FEATURE_ATTR.current_feature_attribute_refined','bq://fxs-gccr-sbd-dev-sandbox.PHISH_FEATURE_ATTR.current_feature_attribute_phish']\n",
    "\n",
    "print(bq_input_uris[0])\n",
    "print(bq_input_uris[1])\n",
    "\n",
    "# for table in jobs_data:\n",
    "#                 print(table)\n",
    "#                 break\n",
    "#                 model_name = jobs_data[table]\n",
    "#                 bq_output_uri = table\n",
    "\n",
    "#                 # Make batch prediction request to the model\n",
    "#                 batch_prediction_job = {\n",
    "#                     \"display_name\": \"batch_prediction_job\",\n",
    "#                     \"model\": model_name,\n",
    "#                     \"input_config\": {\n",
    "#                         \"instances_format\": \"bigquery\",\n",
    "#                         \"bigquery_source\": {\"input_uri\": bq_input_uris[i]}\n",
    "#                     },\n",
    "#                     \"output_config\": {\n",
    "#                         \"predictions_format\": \"bigquery\",\n",
    "#                         \"bigquery_destination\": {\"output_uri\": bq_output_uri},\n",
    "#                     },\n",
    "#                     \"dedicated_resources\": {\n",
    "#                         \"machine_spec\": {\n",
    "#                             \"machine_type\": \"n1-standard-8\"\n",
    "#                         },\n",
    "#                         \"starting_replica_count\": 20,\n",
    "#                         \"max_replica_count\": 200\n",
    "#                     },\n",
    "#                     \"explanation_spec\": {  # This is the additional field for feature attributions\n",
    "#                         \"parameters\": {\n",
    "#                             \"sampled_shapley_attribution\": {  # Using Sampled Shapley feature attribution\n",
    "#                                 \"path_count\": 10  # The number of feature permutations for Shapley values\n",
    "#                             }\n",
    "#                         }\n",
    "#                     },\n",
    "#                     \"generate_explanation\": True  # Setting this to true to enable explanations\n",
    "#                 }\n",
    "#                 logging.info(f\"batch preds made for {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37eadc-0726-4847-9a34-e452c5fb362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_preds() -> None:\n",
    "    import datetime\n",
    "    from google.cloud import aiplatform_v1, bigquery, storage\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    \n",
    "    \n",
    "    def get_models(my_type): # my_type = [PhishPoint, Model_Finale]\n",
    "        api_endpoint = f\"{location}-aiplatform.googleapis.com\"\n",
    "        client = aiplatform_v1.services.model_service.ModelServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
    "        model_name = None\n",
    "        \n",
    "        #Loops through all models, finds PhishPoint\n",
    "        for model in client.list_models(parent=f\"projects/{project_id}/locations/{location}\"):\n",
    "            if model.display_name == my_type:\n",
    "                model_name = model.name\n",
    "\n",
    "\n",
    "        if model_name is None:\n",
    "            raise ValueError(f\"Model with display name {my_type} not found.\")\n",
    "            \n",
    "        return model_name\n",
    "    \n",
    "    def batch_jobs(bq_input_uri,jobs): # my_type = [PHISHING_BATCH, ACTUAL_SCALED_BATCH]\n",
    "        # Set up authentication and project variables\n",
    "        \n",
    "\n",
    "        # Make batch prediction request to the model\n",
    "        api_endpoint = f\"{location}-aiplatform.googleapis.com\"\n",
    "        client = aiplatform_v1.services.job_service.JobServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
    "\n",
    "        responses = []\n",
    "        for job in jobs:\n",
    "            model_name = jobs[job]\n",
    "            \n",
    "            if job =='PHISHING_BATCH':\n",
    "                bq_output_uri = f'bq://fxs-gccr-sbd-dev-sandbox.PHISHING_BATCH'\n",
    "                bq_input_uri =  'bq://fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.phish_model_bq_input'\n",
    "            else:\n",
    "                bq_output_uri = f'bq://fxs-gccr-sbd-dev-sandbox.REFINED_BATCH'\n",
    "                bq_input_uri =  'bq://fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.refined_model_bq_input'\n",
    "\n",
    "            batch_prediction_job = {\n",
    "                \"display_name\": \"batch_prediction_job\",\n",
    "                \"model\": model_name,\n",
    "                \"input_config\": {\n",
    "                    \"instances_format\": \"bigquery\",\n",
    "                    \"bigquery_source\": {\"input_uri\": bq_input_uri}\n",
    "                },\n",
    "                \"output_config\": {\n",
    "                    \"predictions_format\": \"bigquery\",\n",
    "                    \"bigquery_destination\": {\"output_uri\": bq_output_uri},\n",
    "                },\n",
    "                \"dedicated_resources\": {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-8\"\n",
    "                    },\n",
    "                    \"starting_replica_count\": 20,\n",
    "                    \"max_replica_count\": 200\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = client.create_batch_prediction_job(\n",
    "                parent=f\"projects/{project_id}/locations/{location}\",\n",
    "                batch_prediction_job=batch_prediction_job\n",
    "            )\n",
    "            \n",
    "            responses.append(response)\n",
    "            \n",
    "        \n",
    "        failed = False\n",
    "        while not failed:\n",
    "            \n",
    "            # check all the responses\n",
    "            finished = 0\n",
    "            for response in responses:\n",
    "                \n",
    "                job_response = client.get_batch_prediction_job(name=response.name)\n",
    "                logging.info(job_response.state.name)\n",
    "                \n",
    "                if job_response.state.name == \"JOB_STATE_SUCCEEDED\":\n",
    "                    logging.info(\"Batch prediction job finished successfully.\")\n",
    "                    finished += 1\n",
    "                elif job_response.state.name == \"JOB_STATE_FAILED\" or job_response.state.name == \"JOB_STATE_CANCELLED\":\n",
    "                    logging.error(f\"Batch prediction job failed with state {job_response.state.name}\")\n",
    "                    failed = True                    \n",
    "                    \n",
    "            # if we have seen all of our jobs finish\n",
    "            if finished == len(responses):\n",
    "                return True\n",
    "            time.sleep(60)\n",
    "        # if one of the jobs have failed\n",
    "        raise ValueError(\"Batch Preds did not run fully\")\n",
    "\n",
    "\n",
    "    def get_tableids(dataset_id): # dataset_id = [PHISHING_BATCH, REFINED_BATCH]\n",
    "        # Create a client object\n",
    "        client = bigquery.Client()\n",
    "        datasets = list(client.list_datasets(project=project_id))\n",
    "        # Select a dataset\n",
    "        for dataset in datasets:\n",
    "            if dataset.dataset_id == dataset_id:\n",
    "                dataset_ref = client.dataset(dataset.dataset_id, project=project_id)\n",
    "                logging.info(str(dataset_ref))\n",
    "                full_dataset = client.get_dataset(dataset_ref)\n",
    "                logging.info(str(full_dataset))\n",
    "                break\n",
    "        #get pred table\n",
    "        tables = client.list_tables(full_dataset)\n",
    "        sorted_tables = sorted(tables, key=lambda table: table.created or table.modified, reverse=True)\n",
    "        most_recent_table = sorted_tables[0].table_id\n",
    "\n",
    "        return most_recent_table\n",
    "    \n",
    "    def return_df(phish_table, refined_table,jobs_data): # dataset_id = [PHISHING_BATCH, REFINED_BATCH]\n",
    "\n",
    "        def make_feature_inputs():\n",
    "            logging.info(\"making feature attr tables....\")\n",
    "\n",
    "            my_client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "            \n",
    "            \n",
    "#             phish_batch = f'fxs-gccr-sbd-dev-sandbox.PHISHING_BATCH.{phish_table}'\n",
    "#             refined_batch = f'fxs-gccr-sbd-dev-sandbox.REFINED_BATCH.{refined_table}'\n",
    "                \n",
    "#             my_query =f\"\"\"\n",
    "#                         BEGIN\n",
    "#                         -- Define the DistinctURLs in a temporary table\n",
    "#                         CREATE TEMP TABLE DistinctURLs AS (\n",
    "#                             SELECT url FROM (\n",
    "#                             (SELECT url\n",
    "#                                 FROM `{phish_batch}`,\n",
    "#                                 UNNEST(predicted_type_code.scores) as score\n",
    "#                                 ORDER BY score DESC\n",
    "#                                 LIMIT 1000)\n",
    "#                             UNION DISTINCT\n",
    "#                             (SELECT url\n",
    "#                                 FROM `{phish_batch}`,\n",
    "#                                 UNNEST(predicted_type_code.scores) as score\n",
    "#                                 ORDER BY score ASC\n",
    "#                                 LIMIT 1000)\n",
    "#                                 UNION DISTINCT\n",
    "#                                 (SELECT url\n",
    "#                                 FROM `{refined_batch}`,\n",
    "#                                 UNNEST(predicted_type_code.scores) as score\n",
    "#                                 ORDER BY score DESC\n",
    "#                                 LIMIT 1000)\n",
    "#                             UNION DISTINCT\n",
    "#                             (SELECT url\n",
    "#                                 FROM `{refined_batch}`,\n",
    "#                                 UNNEST(predicted_type_code.scores) as score\n",
    "#                                 ORDER BY score ASC\n",
    "#                                 LIMIT 1000)\n",
    "#                             )\n",
    "#                             );\n",
    "                        \n",
    "#   -- Create the phish table using the DistinctURLs temporary table\n",
    "#                         CREATE OR REPLACE TABLE `fxs-gccr-sbd-dev-sandbox.PHISH_FEATURE_ATTR.current_feature_attribute_phish` AS\n",
    "#                             SELECT d.url, \n",
    "#                             p.url AS old_url,\n",
    "#                             p.having_ip_address AS having_ip_address,\n",
    "#                             p.abnormal_url AS abnormal_url,\n",
    "#                             p.count_dot AS count_dot,\n",
    "#                             p.count_www AS count_www,\n",
    "#                             p.count_atrate AS count_atrate,\n",
    "#                             p.count_https AS count_https,\n",
    "#                             p.short_url AS short_url,\n",
    "#                             p.count_http AS count_http,\n",
    "#                             p.count_ques AS count_ques,\n",
    "#                             p.count_per AS count_per,\n",
    "#                             p.count_hyphen AS count_hyphen,\n",
    "#                             p.count_equal AS count_equal,\n",
    "#                             p.url_length AS url_length,\n",
    "#                             p.hostname_length AS hostname_length,\n",
    "#                             p.sus_url AS sus_url,\n",
    "#                             p.count_digits AS count_digits,\n",
    "#                             p.count_letters AS count_letters,\n",
    "#                             p.count_dir AS count_dir,\n",
    "#                             p.count_embed_domain AS count_embed_domain,\n",
    "#                             p.fd_length AS fd_length,\n",
    "#                             p.tld_length AS tld_length,\n",
    "#                             p.index AS index\n",
    "\n",
    "#                             FROM DistinctURLs d \n",
    "#                             JOIN `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.phish_model_bq_input` p\n",
    "#                             ON d.url = p.url;\n",
    "\n",
    "#                         -- Create the refined table using the DistinctURLs temporary table\n",
    "#                         CREATE OR REPLACE TABLE `fxs-gccr-sbd-dev-sandbox.REFINED_FEATURE_ATTR.current_feature_attribute_refined` AS\n",
    "#                         SELECT d.url, \n",
    "#                             p.abnormal_url AS abnormal_url,\n",
    "#                             p.count_dot AS count_dot,\n",
    "#                             p.count_www AS count_www,\n",
    "#                             p.count_atrate AS count_atrate,\n",
    "#                             p.count_https AS count_https,\n",
    "#                             p.count_ques AS count_ques,\n",
    "#                             p.count_per AS count_per,\n",
    "#                             p.count_hyphen AS count_hyphen,\n",
    "#                             p.count_equal AS count_equal,\n",
    "#                             p.url_length AS url_length,\n",
    "#                             p.hostname_length AS hostname_length,\n",
    "#                             p.sus_words AS sus_words,\n",
    "#                             p.count_digits AS count_digits,\n",
    "#                             p.count_letters AS count_letters,\n",
    "#                             p.count_dir AS count_dir,\n",
    "#                             p.count_embed_domain AS count_embed_domain,\n",
    "#                             p.fd_length AS fd_length,\n",
    "#                             p.special_chars AS special_chars,\n",
    "#                             p.nonstandard_chars AS nonstandard_chars,\n",
    "#                             p.risky_ext AS risky_ext,\n",
    "#                             p.ratio_digits AS ratio_digits,\n",
    "#                             p.double_slash AS double_slash,\n",
    "#                             p.sus_query AS sus_query,\n",
    "#                             p.sus_tld AS sus_tld,\n",
    "#                             p.adult_words AS adult_words,\n",
    "#                             p.sus_port AS sus_port,\n",
    "#                             p.ip_address AS ip_address,\n",
    "#                             p.subdomain_count AS subdomain_count,\n",
    "#                             p.url AS old_url,\n",
    "#                             p.index AS index,\n",
    "#                             p.domain AS domain,\n",
    "#                             p.path AS path,\n",
    "#                             p.tld AS tld\n",
    "\n",
    "#                             FROM DistinctURLs d \n",
    "#                             JOIN `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.refined_model_bq_input` p\n",
    "#                             ON d.url = p.url;\n",
    "\n",
    "#                         -- Drop the temporary table\n",
    "#                         DROP TABLE DistinctURLs;\n",
    "#                         END;\n",
    "#             \"\"\"\n",
    "#             print('finished making tables')\n",
    "#             my_client.query(my_query)\n",
    "#             logging.info(\"done....!\")\n",
    "\n",
    "            api_endpoint = f\"{location}-aiplatform.googleapis.com\"\n",
    "            client = aiplatform_v1.services.job_service.JobServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
    "            bq_input_uris = ['bq://fxs-gccr-sbd-dev-sandbox.PHISH_FEATURE_ATTR.current_feature_attribute_phish','bq://fxs-gccr-sbd-dev-sandbox.REFINED_FEATURE_ATTR.current_feature_attribute_refined']\n",
    "            i = 0\n",
    "            print('starting making batch preds')\n",
    "\n",
    "            for table in jobs_data:\n",
    "                model_name = jobs_data[table]\n",
    "                bq_output_uri = table\n",
    "\n",
    "                # Make batch prediction request to the model\n",
    "                batch_prediction_job = {\n",
    "                    \"display_name\": \"batch_prediction_job\",\n",
    "                    \"model\": model_name,\n",
    "                    \"input_config\": {\n",
    "                        \"instances_format\": \"bigquery\",\n",
    "                        \"bigquery_source\": {\"input_uri\": bq_input_uris[i]}\n",
    "                    },\n",
    "                    \"output_config\": {\n",
    "                        \"predictions_format\": \"bigquery\",\n",
    "                        \"bigquery_destination\": {\"output_uri\": bq_output_uri},\n",
    "                    },\n",
    "                    \"dedicated_resources\": {\n",
    "                        \"machine_spec\": {\n",
    "                            \"machine_type\": \"n1-standard-8\"\n",
    "                        },\n",
    "                        \"starting_replica_count\": 20,\n",
    "                        \"max_replica_count\": 200\n",
    "                    },\n",
    "                    \"explanation_spec\": {  # This is the additional field for feature attributions\n",
    "                        \"parameters\": {\n",
    "                            \"sampled_shapley_attribution\": {  # Using Sampled Shapley feature attribution\n",
    "                                \"path_count\": 10  # The number of feature permutations for Shapley values\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"generate_explanation\": True  # Setting this to true to enable explanations\n",
    "                }\n",
    "                logging.info(f\"batch preds made for {table}\")\n",
    "                \n",
    "                response = client.create_batch_prediction_job(\n",
    "                    parent=f\"projects/{project_id}/locations/{location}\",\n",
    "                    batch_prediction_job=batch_prediction_job\n",
    "                )\n",
    "                i = i+1\n",
    "                    \n",
    "            return\n",
    "        \n",
    "        def get_predicted_urls():\n",
    "            # Initialization of the Query\n",
    "\n",
    "            my_client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "            # The Actual BigQuery Search\n",
    "            my_query = f\"\"\"SELECT \n",
    "            t1.index as index,  \n",
    "            t1.url as url,\n",
    "            t1.score as phish_score,\n",
    "            t2.score as murl_score\n",
    "            FROM (\n",
    "            SELECT *\n",
    "                FROM `fxs-gccr-sbd-dev-sandbox.PHISHING_BATCH.{phish_table}`,\n",
    "                UNNEST(predicted_type_code.scores) as score\n",
    "                WHERE score >= .995) \n",
    "                t1\n",
    "                JOIN (\n",
    "                    SELECT *\n",
    "                    FROM `fxs-gccr-sbd-dev-sandbox.REFINED_BATCH.{refined_table}`,\n",
    "                    UNNEST(predicted_type_code.scores) as score\n",
    "                    WHERE score >= .995) \n",
    "                    t2\n",
    "                    ON \n",
    "                    t1.index = t2.index AND\n",
    "                    t1.url = t2.url \n",
    "                    ORDER BY t1.index\"\"\"\n",
    "            \n",
    "            ### BigQuery Test Job Setup\n",
    "            my_config = bigquery.QueryJobConfig()\n",
    "            my_config.dry_run = True\n",
    "            my_test = my_client.query(my_query, job_config=my_config)\n",
    "            logging.info(f\"This job will take ~{my_test.total_bytes_processed/(10**9):.2f} gigabytes or {((my_test.total_bytes_processed/(10**9))/1024)*5:.2f} dollars to run\")\n",
    "            ###\n",
    "\n",
    "            # Returns the Client and Query to then be ran \n",
    "            return my_client, my_query\n",
    "        \n",
    "\n",
    "        # test the query, check and make sure it doesn't cost $13.7 billion\n",
    "        predicted_urls_client, predicted_urls_query = get_predicted_urls()\n",
    "        # run the query\n",
    "        logging.info(f\"Combining dfs....\")\n",
    "        df = predicted_urls_client.query(predicted_urls_query).to_dataframe()\n",
    "        df = df.iloc[1::2]  # this line is because of bigquery returning one row for malicious and one row for begign scores\n",
    "        logging.info(f\"Combined!\")\n",
    "\n",
    "        logging.info(f\"Making feature attribution batch pred tables....\")\n",
    "        make_feature_inputs()\n",
    "        logging.info(f\"Done....!\")\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    \n",
    "    start = time.time()\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    project_id = 'fxs-gccr-sbd-dev-sandbox'\n",
    "    todayDate = datetime.date.today()\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    location = 'us-west1'\n",
    "    phishpoint_model = get_models(\"PhishPoint_WIndex\")\n",
    "    refined_model = get_models(\"Repurged_Scaled\")\n",
    "    logging.info(\"got models\")\n",
    "    \n",
    "\n",
    "    client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "    temp_table = \"fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_megapreprocess_output_temp\"\n",
    "   \n",
    "    # the above is done so that the bigquery table we pull from only has the most recent week's results\n",
    "    # batch_jobs(f\"bq://{temp_table}\", {\"PHISHING_BATCH\":phishpoint_model, \"REFINED_BATCH\":refined_model}) \n",
    "    \n",
    "    logging.info(\"getting phish id\")\n",
    "    \n",
    "    most_recent_phish_id = get_tableids(\"PHISHING_BATCH\")\n",
    "    logging.info(\"got phish id\")\n",
    "    logging.info(\"phish done!\")\n",
    "\n",
    "    logging.info(\"getting refined id\")\n",
    "    \n",
    "    most_recent_refined_id = get_tableids(\"REFINED_BATCH\")\n",
    "    logging.info(\"got refined id\")\n",
    "    scaled_df = return_df(most_recent_phish_id, most_recent_refined_id, {\"bq://fxs-gccr-sbd-dev-sandbox.PHISH_FEATURE_ATTR\":phishpoint_model, \"bq://fxs-gccr-sbd-dev-sandbox.REFINED_FEATURE_ATTR\":refined_model})\n",
    "    logging.info(\"got combined df and started batch preds for feature \")\n",
    "\n",
    "    scaled_df['week_of'] = pd.to_datetime(previous_monday)\n",
    "    scaled_df.to_gbq(destination_table=f'umrf_murl_v2_results.TEST_batch_predictions_output', \n",
    "        project_id='fxs-gccr-sbd-dev-sandbox', if_exists='append')\n",
    "\n",
    "    end = time.time()\n",
    "    timer = f\"total time to run: {end-start}\"\n",
    "    logging.info(timer)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880488c0-ecf7-41aa-805e-d5d5296e300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_batch_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139a04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb832f10-f4a5-4b28-ba22-307f84ba0c0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a id='user_count'></a>\n",
    "# <center> Step #7: User Count Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d8e70-50aa-4c8a-aeb1-007a30f3ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_count_scoring() -> None:\n",
    "    import datetime\n",
    "    from google.cloud import bigquery, storage\n",
    "    import logging\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    \n",
    "    def dataframe_upload(table, monday, df):\n",
    "        client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "        query = f\"DELETE FROM fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.{table} WHERE week_of = '{monday}'\"\n",
    "        client.query(query)\n",
    "        df.to_gbq(destination_table=f'umrf_murl_v2_results.{table}',\n",
    "                project_id='fxs-gccr-sbd-dev-sandbox', chunksize=1_000_000, if_exists='replace')\n",
    "    # this function allows us to quickly and easily normalize the scores\n",
    "    # according to the drop off that we desire towards the 50 mark\n",
    "    start = time.time()\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    ######\n",
    "    client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "    todayDate = datetime.date.today()\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    query = f\"SELECT * FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_exploded_query_w_users` WHERE week_of = '{previous_monday}'\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "    df = df.fillna(0)\n",
    "    #df = pd.read_csv(input_file)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    normalized_values = 1 - scaler.fit_transform(df[['user_count']])\n",
    "    df['user_count_score'] = normalized_values\n",
    "    df['user_count_score'].fillna(0, inplace=True)\n",
    "   \n",
    "    dataframe_upload('TEST_user_scoring',previous_monday,df )\n",
    "    \n",
    "    end = time.time()\n",
    "    timer = f\"total time to run: {end-start}\"\n",
    "    logging.info(timer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_user_count_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56db84b-ab23-4300-8d57-b22599151436",
   "metadata": {},
   "source": [
    "## <a id='combine'></a>\n",
    "# <center> Step #8: Combine Outputs from Batch Predictions and User Count Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fa437-7ea6-4675-bd31-b74e82e57e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_user_dfs() -> None:\n",
    "    import datetime\n",
    "    from google.cloud import bigquery, storage\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    \n",
    "    def dataframe_upload(table, monday, df):\n",
    "        client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "        query = f\"DELETE FROM fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.{table} WHERE week_of = '{monday}'\"\n",
    "        client.query(query)\n",
    "        df.to_gbq(destination_table=f'umrf_murl_v2_results.{table}',\n",
    "                project_id='fxs-gccr-sbd-dev-sandbox', chunksize=1_000_000, if_exists='replace')\n",
    "    # this function allows us to quickly and easily normalize the scores\n",
    "\n",
    "    \n",
    "    start = time.time()\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    #batch predictions df\n",
    "    client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "    todayDate = datetime.date.today()\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    batch_query = f\"SELECT * FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_batch_predictions_output` WHERE week_of = '{previous_monday}'\"\n",
    "\n",
    "    main_df = client.query(batch_query).to_dataframe()\n",
    "    #user score df\n",
    "    user_score_query = f\"SELECT * FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_user_scoring` WHERE week_of = '{previous_monday}'\"\n",
    "    second_df = client.query(user_score_query).to_dataframe()\n",
    "    \n",
    "    # main_df.set_index('index', inplace=True)\n",
    "    # print(main_df.columns)\n",
    "    # second_df.set_index('index', inplace=True)\n",
    "    # print(second_df.columns)\n",
    "    #combined_df = main_df.merge(second_df.drop(columns=set(main_df.columns)-{'index'},errors='ignore'), on='index', how='left')\n",
    "    # For 'user_score'\n",
    "    main_df['user_count_score'] = main_df.index.map(second_df['user_count_score'].to_dict())   \n",
    "    main_df['user_count'] = main_df.index.map(second_df['user_count'].to_dict())\n",
    "    main_df['users_list'] = main_df.index.map(second_df['users_list'].to_dict())\n",
    "    main_df['index'] = main_df['index'].astype('int')\n",
    "    \n",
    "    dataframe_upload('TEST_combine_dfs',previous_monday,main_df )\n",
    "\n",
    "    end = time.time()\n",
    "    timer = f\"total time to run: {end-start}\"\n",
    "    logging.info(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2dfc3-89a7-44c5-b414-d19f02fb7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_user_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951cf7db-2b58-4c69-817a-bb123ab59245",
   "metadata": {},
   "source": [
    "## <a id='RDAP'></a>\n",
    "# <center> Step #9: RDAP Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53a11c-edb4-4f48-9720-5729558dea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdap_scored_df() -> None:\n",
    "    import ast\n",
    "    import concurrent.futures\n",
    "    import datetime\n",
    "    from google.cloud import bigquery, storage\n",
    "    import logging\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import sys\n",
    "    import threading\n",
    "    import time\n",
    "    import tldextract\n",
    "\n",
    "\n",
    "    def read_dict_from_gcs(blob):\n",
    "        # Download the file as a string\n",
    "        file_content = blob.download_as_text()\n",
    "\n",
    "        # Parse the string into a dictionary using ast\n",
    "        dictionary = ast.literal_eval(file_content)\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    def upload_dict_to_gcs(dictionary, bucket_name, destination_file_name):\n",
    "        # Convert the dictionary to a string\n",
    "        from google.cloud import storage\n",
    "        dictionary_string = str(dictionary)\n",
    "\n",
    "        # Instantiates a client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Retrieves the bucket\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "        # Uploads the dictionary string to GCS as a .txt file\n",
    "        blob = bucket.blob(destination_file_name)\n",
    "        blob.upload_from_string(dictionary_string, content_type='text/plain')\n",
    "\n",
    "        print(f\"Dictionary uploaded to GCS bucket {bucket_name} as {destination_file_name}.\")\n",
    "\n",
    "\n",
    "    def get_basic_rdap(attempts):\n",
    "        try:\n",
    "            rdap_bootstrap_url = \"https://data.iana.org/rdap/dns.json\"\n",
    "            \n",
    "            # TODO: No proxy's are being exported, so no proxy is ever used.\n",
    "            # Get proxy settings from environment variables (if there are any)\n",
    "            proxies = {\n",
    "                'http': os.environ.get('http_proxy'),\n",
    "                'https': os.environ.get('https_proxy')\n",
    "            }\n",
    "            for i in range(attempts):\n",
    "                # Fetch RDAP bootstrap data using proxy settings and 10 seconds timeout\n",
    "                try:\n",
    "                    bootstrap_data = requests.get(rdap_bootstrap_url, proxies=proxies, timeout=10).json()\n",
    "                    logging.info(f'RDAP Successful {i}')\n",
    "                    return bootstrap_data\n",
    "                except requests.exceptions.Timeout:\n",
    "                    logging.warning(\"Timeout error fetching RDAP bootstrap data.\")\n",
    "                except requests.exceptions.TooManyRedirects:\n",
    "                    logging.warning(\"Too many redirects when fetching RDAP bootstrap data.\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logging.warning(f\"Error fetching RDAP bootstrap data: {e}\")\n",
    "                except ValueError:\n",
    "                    logging.warning(\"Invalid JSON received from RDAP bootstrap service.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\" EXCEPTION: {e}\")\n",
    "            logging.error(\"Error grabbing basic rdap JSON\")\n",
    "            sys.exit(61)\n",
    "    \n",
    "\n",
    "    def get_rdap_url(domain, bootstrap_data):\n",
    "        try:\n",
    "            # Iterate through each entry in the 'services' list in the JSON response data\n",
    "            for entry in bootstrap_data['services']:\n",
    "                # Iterate through each domain pattern in the first element of the entry\n",
    "                for domain_pattern in entry[0]:\n",
    "                    # If 'domain' ends with the domain pattern\n",
    "                    if domain.endswith(domain_pattern):\n",
    "                        # Return the first RDAP server listed in the second element of the entry\n",
    "                        # Can be queried for info about the domain\n",
    "                        return entry[1][0] # return the first RDAP server URL for this TLD\n",
    "            # Return None if no matching RDAP server is found\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\" EXCEPTION: {e}\")\n",
    "            logging.error(f\"Error getting rdap_url from {domain}\")\n",
    "            sys.exit(62)\n",
    "    \n",
    "\n",
    "    def score_domain(domain_age):\n",
    "        max_score = 1\n",
    "        k = 0.004\n",
    "        min_k = 0.0001\n",
    "        max_k = 0.01\n",
    "        k = max(min_k, min(max_k, k))\n",
    "        \n",
    "        if domain_age is None or pd.isna(domain_age):\n",
    "            base_line = max_score / 2\n",
    "            return np.round(np.interp(base_line, [0, max_score], [0, 1]), 2)\n",
    "        \n",
    "        if domain_age < 0:\n",
    "            domain_age = 0\n",
    "\n",
    "        score = max_score * math.exp(-k * domain_age)\n",
    "        score = max(0, min(max_score, score))\n",
    "        return np.round(np.interp(score, [0, max_score], [0, 1]), 2)\n",
    "\n",
    "\n",
    "    def domain_getter(url):        \n",
    "        url_tld = tldextract.extract(url).suffix\n",
    "        domain_name = tldextract.extract(url).domain\n",
    "        full_domain = domain_name + '.' + url_tld\n",
    "        return full_domain\n",
    "    \n",
    "\n",
    "    def domain_func(domain, bootstrap_data):\n",
    "        try:\n",
    "            nonlocal no_tld_expert_failures, http_failures, no_events_failures, no_registration_failures, misc_failures, request_counter, already_cached, rdap_semaphore, domain_cache\n",
    "                    \n",
    "            if domain in domain_cache:\n",
    "\n",
    "                creation_date = datetime.datetime.strptime(domain_cache[domain], '%Y-%m-%d')\n",
    "                logging.info(f'Creation date: {creation_date}')\n",
    "                # if we have made it to this point, we have a creation date\n",
    "                current_date = datetime.datetime.today()\n",
    "                age_in_days = (current_date - creation_date).days\n",
    "\n",
    "                # and we retrieve a final score using a \"score_domain function\"\n",
    "                already_cached += 1\n",
    "                \n",
    "                logging.info(f'Already Cached: {already_cached}')\n",
    "                return domain, score_domain(age_in_days)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception for domain {domain}: {e}')\n",
    "            return domain, None\n",
    "\n",
    "        \n",
    "        # If the domain starts with 'www.', remove the prefix\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        # Get the RDAP server URL for the domain\n",
    "        rdap_url = get_rdap_url(domain, bootstrap_data)\n",
    "        \n",
    "        if not rdap_url:\n",
    "            no_tld_expert_failures += 1\n",
    "            if no_tld_expert_failures % 10 == 0:\n",
    "                logging.warning(f\"no_tld_expert_failures: {no_tld_expert_failures}\")\n",
    "            logging.warning(f\"No tld expert found for {domain}\")\n",
    "            return domain, None\n",
    "        # Construct the RDAP info URL\n",
    "        if rdap_url[-1] == \"/\":\n",
    "            rdap_info_url = f\"{rdap_url}domain/{domain}\"\n",
    "            logging.info(f\"didnt need slash\")\n",
    "        else:\n",
    "            rdap_info_url = f\"{rdap_url}/domain/{domain}\"\n",
    "            logging.info(f\"needed slash\")\n",
    "        \n",
    "        try:\n",
    "            logging.info(f'Trying rdap_semaphore: {rdap_semaphore}')\n",
    "            # Send a GET request to the RDAP info URL with a 10 seconds timeout\n",
    "            with rdap_semaphore:\n",
    "                request_counter += 1\n",
    "                if request_counter % 1000 == 0:\n",
    "                    logging.info(f\"{request_counter} requests done {time.time()}\")\n",
    "                response = requests.get(rdap_info_url, timeout=10)\n",
    "                \n",
    "            # If the request is successful, return the JSON response data\n",
    "            if not response.ok:\n",
    "                http_failures += 1\n",
    "                if http_failures % 10 == 0:\n",
    "                    logging.error(f\"Error fetching RDAP info for {domain} from {rdap_info_url}: HTTP {response.status_code}. Reason: {response.reason}\")\n",
    "                    print(f\"http_failures: {http_failures}\")\n",
    "                return domain, None\n",
    "            response = response.json()\n",
    "        except Exception as e:\n",
    "            misc_failures += 1\n",
    "            if misc_failures % 10 == 0:\n",
    "                logging.warning(f\"misc_failures: {misc_failures}\")\n",
    "                logging.error(f\"exception during request getting {e} from domain: {domain} to url {rdap_info_url}\")\n",
    "            return domain, None\n",
    "        \n",
    "        if 'events' not in response:\n",
    "            no_events_failures += 1\n",
    "            if no_events_failures % 1000 == 0:\n",
    "                logging.error(f\"no_events_failures: {no_events_failures}\")\n",
    "                logging.error(f\"No events from {rdap_info_url}\")\n",
    "            return domain, None\n",
    "        creation_date = None\n",
    "        for event in response['events']:\n",
    "            if event['eventAction'] == 'registration':\n",
    "                creation_date = datetime.datetime.strptime(event['eventDate'][0:10], '%Y-%m-%d')\n",
    "                domain_cache[domain] = creation_date.strftime('%Y-%m-%d')\n",
    "                break\n",
    "\n",
    "        # if we didn't find a creation date\n",
    "        if creation_date is None:\n",
    "            no_registration_failures += 1\n",
    "            if no_registration_failures % 1000 == 0:\n",
    "                print(f\"no_registration_failures: {no_registration_failures}\")\n",
    "            logging.warning(f\"No date found from {rdap_info_url}\")\n",
    "            return domain, None\n",
    "\n",
    "    # if we have made it to this point, we have a creation date\n",
    "        current_date = datetime.datetime.today()\n",
    "        age_in_days = (current_date - creation_date).days\n",
    "\n",
    "    # and we retrieve a final score using a \"score_domain function\"\n",
    "\n",
    "        return domain, score_domain(age_in_days)\n",
    "        \n",
    "      \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    bucket = \"suspicious_user_bucket\"\n",
    "    file_name = \"rdap_cache/rdap_domains.txt\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    domain_cache = {}\n",
    "    if blob.exists():\n",
    "        domain_cache = read_dict_from_gcs(blob)\n",
    "    logging.info(\"Got Cache\")\n",
    "    \n",
    "    no_tld_expert_failures = http_failures = no_events_failures = no_registration_failures = misc_failures = request_counter = already_cached = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "    todayDate = datetime.date.today()\n",
    "    previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "    logging.info(\"sending query\")\n",
    "    query = f\"\"\"SELECT * \n",
    "    FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_combine_dfs` \n",
    "    WHERE week_of = '{previous_monday}'\n",
    "    limit 500000\"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "    df_len = len(df)\n",
    "    logging.info(f\"Loaded {df_len} rows\")\n",
    "    score_dict = {}\n",
    "    df['rdap_score'] = None\n",
    "    df = df.sample(frac=1)\n",
    "    logging.info(\"Getting domains\")\n",
    "    df['domain'] = df['url'].apply(domain_getter)\n",
    "\n",
    "    domains = set(df['domain'])\n",
    "    logging.info(f\"{len(domains)} domains\")\n",
    "    bootstrap_data = get_basic_rdap(attempts=10)\n",
    "    if bootstrap_data is None:\n",
    "        logging.critical(\"PLEASE HELP RDAP FAILED 10 TIMES IN A ROW THIS IS REALLY REALLY BAD\")\n",
    "        return \n",
    "    rdap_semaphore = threading.Semaphore(4)\n",
    "    logging.info(\"Starting threads\")\n",
    "    \n",
    "    ## Make threads loop through domains and make requests. \n",
    "    # creating a dictionary with keys being domain and value being the creation date of the domain\n",
    "    # once all are done we can make new column based on responses\n",
    "    answers = {}\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Mapping index and url to process_url function\n",
    "        futures = {executor.submit(domain_func, domain, bootstrap_data): domain for domain in domains}\n",
    "        #futures = {executor.submit(process_url, index, row['url']): (index, row['url']) for index, row in df.iterrows()}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            domain, result = future.result()\n",
    "            answers[domain] = result\n",
    "    df['rdap_score'] = df.domain.map(answers)\n",
    "    \n",
    "\n",
    "    bucket = \"suspicious_user_bucket\"\n",
    "    file_name = \"rdap_cache/rdap_domains.txt\"\n",
    "\n",
    "    # Call the function to upload the dictionary to GCS\n",
    "    upload_dict_to_gcs(domain_cache, bucket, file_name)\n",
    "    print(df.info())\n",
    "    df.to_gbq(destination_table=f'umrf_murl_v2_results.TEST_rdap', \n",
    "        project_id='fxs-gccr-sbd-dev-sandbox', if_exists='replace')\n",
    "    end = time.time()\n",
    "    timer = f\"total time to run: {end-start}\"\n",
    "    logging.info(timer)\n",
    "    logging.info(f\"tld expert failures: {no_tld_expert_failures}\")\n",
    "    logging.info(f\"http failures: {http_failures}\")\n",
    "    logging.info(f\"No events found failures: {no_events_failures}\")\n",
    "    logging.info(f\"no registration dates: {no_registration_failures}\")\n",
    "    logging.info(f\"Misc. Failures {misc_failures}\")\n",
    "    total_failures = no_tld_expert_failures + http_failures + no_events_failures + no_registration_failures + misc_failures\n",
    "    logging.info(f\"Total Failures: {total_failures}\")\n",
    "    logging.info(f\"requests sent: {request_counter}\")\n",
    "    logging.info(f\"Domains already cached {already_cached}\")\n",
    "    try:\n",
    "        logging.info(f\"Failure rate: {total_failures / len(domains)}\")\n",
    "    except ZeroDivisionError:\n",
    "        logging.info(\"failure rate: Undefined no requests sent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2bb56b-5aa0-402f-b8fe-8eb4d9c7f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdap_scored_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ce5e62-b26d-483c-a5d5-8b2cedac25ba",
   "metadata": {},
   "source": [
    "## <a id='total'></a>\n",
    "# <center> Step #10: Find Total Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a35d7f-3a5e-4db0-a6b3-6d8c894516ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_url_by_domain() -> None:\n",
    "    #sift through input file, take top 3500 URLs\n",
    "    import datetime\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import collections\n",
    "    from collections import Counter\n",
    "    import tldextract\n",
    "    from typing import NamedTuple\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import gcsfs\n",
    "    import numpy as np\n",
    "    from google.cloud import bigquery, storage\n",
    "    import re\n",
    "    \n",
    "    def dataframe_upload(table, monday, df):\n",
    "        client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "        query = f\"DELETE FROM fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.{table} WHERE week_of = '{monday}'\"\n",
    "        client.query(query)\n",
    "        df.to_gbq(destination_table=f'umrf_murl_v2_results.{table}',\n",
    "                project_id='fxs-gccr-sbd-dev-sandbox', chunksize=1_000_000, if_exists='replace')\n",
    "\n",
    "    def domain_purger(url):\n",
    "        try:        \n",
    "            url_sub = tldextract.extract(url).subdomain.split('.')[-1]\n",
    "            if len(url_sub) > 0:\n",
    "                url_sub = url_sub + '.' \n",
    "            domain_name = tldextract.extract(url).domain\n",
    "\n",
    "            url_tld = tldextract.extract(url).suffix\n",
    "\n",
    "            full_domain = url_sub + domain_name + '.' + url_tld\n",
    "            return full_domain\n",
    "        except:\n",
    "            logging.critical(\"Failed to purge similar urls, skipping this part!\")\n",
    "\n",
    "    def high_scores_getter(df, total_urls):\n",
    "        df = df.copy()\n",
    "        # Get the top results and ensure unique selections\n",
    "        selected_indices = set()\n",
    "        top_phish_df = df.nlargest(int(total_urls * .2), 'total_phish_score')\n",
    "        selected_indices.update(top_phish_df.index)\n",
    "        df.drop(index=list(selected_indices), inplace=True)\n",
    "\n",
    "\n",
    "        selected_indices = set()\n",
    "        top_total_df = df.nlargest(int(total_urls * .2), 'total_score')\n",
    "        selected_indices.update(top_total_df.index)\n",
    "        df.drop(index=list(selected_indices), inplace=True)\n",
    "\n",
    "\n",
    "        selected_indices = set()\n",
    "        logging.info(df['tld_count'].value_counts())\n",
    "        top_murl_suspicious_tld_df = df[df['tld_count'] > 0].nlargest(int(total_urls * .15), 'murl_score')\n",
    "        selected_indices.update(top_murl_suspicious_tld_df.index)\n",
    "        df.drop(index=list(selected_indices), inplace=True)\n",
    "\n",
    "\n",
    "        selected_indices = set()\n",
    "        logging.info(df['suspicious_keyword'].value_counts())\n",
    "        top_murl_suspicious_keyword_df = df[df['suspicious_keyword'] > 0].nlargest(int(total_urls * .15), 'murl_score')\n",
    "        selected_indices.update(top_murl_suspicious_keyword_df.index)\n",
    "        df.drop(index=list(selected_indices), inplace=True)\n",
    "\n",
    "\n",
    "        selected_indices = set()\n",
    "        logging.info(df['adult_sus_words'].value_counts())\n",
    "        top_murl_adult_sus_keyword_df = df[df['adult_sus_words'] > 0].nlargest(int(total_urls * .15), 'murl_score')\n",
    "        selected_indices.update(top_murl_adult_sus_keyword_df.index)\n",
    "        df.drop(index=list(selected_indices), inplace=True)\n",
    "\n",
    "        top_murl_df = df.nlargest(int(total_urls * .15), 'murl_score')\n",
    "\n",
    "        logging.info(\"Got small dataframes\")\n",
    "        # Concatenate the DataFrames\n",
    "        high_scores_df = pd.concat([top_phish_df, top_total_df, top_murl_suspicious_tld_df, top_murl_suspicious_keyword_df, top_murl_df, top_murl_adult_sus_keyword_df])\n",
    "        logging.info(\"put dataframes together\")\n",
    "        return high_scores_df\n",
    "\n",
    "\n",
    "    try:\n",
    "        logging.getLogger().setLevel(logging.INFO)\n",
    "        now = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "        client = bigquery.Client(project=\"fxs-gccr-sbd-dev-sandbox\")\n",
    "        todayDate = datetime.date.today()\n",
    "        previous_monday = todayDate + datetime.timedelta(days=-todayDate.weekday(), weeks=-1)\n",
    "        query = f\"SELECT * FROM `fxs-gccr-sbd-dev-sandbox.umrf_murl_v2_results.TEST_rdap` WHERE week_of = '{previous_monday}'\"\n",
    "        logging.info(\"querying...\")\n",
    "        full_df = client.query(query).to_dataframe()\n",
    "        logging.info(\"successfully queried\")\n",
    "        logging.info(f'This is the length before purging similar urls: {len(full_df)}')\n",
    "        full_df['url_purger'] = full_df['url'].apply(domain_purger)\n",
    "\n",
    "\n",
    "        full_df.drop_duplicates(subset='url_purger', inplace=True)\n",
    "        logging.info(f'This is the length after purging similar urls: {len(full_df)}')\n",
    "        full_df.drop(columns=['url_purger'], inplace=True)\n",
    "        logging.info(\"purge ended\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        full_df['rdap_score'].fillna(0, inplace=True)\n",
    "        full_df['total_phish_score'] = full_df['phish_score'] + full_df['rdap_score'] + full_df['user_count_score']\n",
    "        full_df['total_score'] = full_df['phish_score'] + full_df['rdap_score'] + full_df['user_count_score'] + full_df['murl_score']\n",
    "        logging.info(\"added scores\")\n",
    "\n",
    "        def count_suspicious_tlds(url):\n",
    "            parts = url.split('.')\n",
    "            tld_counts = collections.Counter(tldextract.extract(part).suffix for part in parts)\n",
    "            del tld_counts['']\n",
    "            return sum(tld_counts.values())\n",
    "\n",
    "\n",
    "        def count_suspicious_keyword(url):\n",
    "            # check for sus keywords in url\n",
    "            suspicious_keywords = [\n",
    "            'login', 'signin', 'password', 'secure', 'account', 'verification', 'validate', 'confirm', 'token', 'update', 'registry', 'payment', 'credit',\n",
    "            'transaction', 'admin', 'service', 'webmaster', 'helpdesk', 'paypal', 'ebay', 'amazon', 'bank', 'wellsfargo', 'chase', 'citi', 'boa', 'fedex', 'microsoft', \n",
    "            '.exe', '.zip', '.rar', '.doc', '.xls', '.pdf', \n",
    "            'free', 'gift', 'promo', 'offer', 'download', '.dll', 'prize', 'reward', 'sweepstakes', 'lottery', 'winner', 'congratulations',\n",
    "            'script', 'stream', 'play', 'game', 'invoke', 'download', 'cdn', 'media', 'video', 'manga'\n",
    "            ]\n",
    "            return sum(keyword in url.lower() for keyword in suspicious_keywords)\n",
    "\n",
    "\n",
    "        def count_adult_sus_words(url):\n",
    "            # check for 'adult' keywords in url\n",
    "            adult_sus_words = [\n",
    "            'camgirl', 'porn', 'xxx',  'pornhub', 'xvideo', 'xhamster', 'nsfw', 'only','feet', 'kink', 'nude', 'leaked', 'boob', 'boobs', 'lesbian', 'blacked', \n",
    "            'leaks', 'fap', 'voyuer'\n",
    "            ]\n",
    "            return sum(keyword in url.lower() for keyword in adult_sus_words)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        full_df['tld_count'] = full_df['url'].apply(count_suspicious_tlds)\n",
    "        full_df['suspicious_keyword'] = full_df['url'].apply(count_suspicious_keyword)\n",
    "        full_df['adult_sus_words'] = full_df['url'].apply(count_adult_sus_words)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        full_df['tld_count'] = scaler.fit_transform(np.array(full_df['tld_count']).reshape(-1, 1))\n",
    "        full_df['total_phish_score'] = scaler.fit_transform(full_df['total_phish_score'].values.reshape(-1, 1))\n",
    "        full_df['total_score'] = scaler.fit_transform(full_df['total_score'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "        logging.info(\"added normalized and added counts\")\n",
    "        shorteners = [\n",
    "            \"bit.ly\", \"goo.gl\", \"shorte.st\", \"go2l.ink\", \"x.co\", \"ow.ly\", \"t.co\", \"tinyurl\",\n",
    "            \"tr.im\", \"is.gd\", \"cli.gs\", \"yfrog.com\", \"migre.me\", \"ff.im\", \"tiny.cc\", \"url4.eu\",\n",
    "            \"twit.ac\", \"su.pr\", \"twurl.nl\", \"snipurl.com\", \"short.to\", \"BudURL.com\", \"ping.fm\",\n",
    "            \"post.ly\", \"Just.as\", \"bkite.com\", \"snipr.com\", \"fic.kr\", \"loopt.us\", \"doiop.com\",\n",
    "            \"short.ie\", \"kl.am\", \"wp.me\", \"rubyurl.com\", \"om.ly\", \"to.ly\", \"bit.do\", \"lnkd.in\",\n",
    "            \"db.tt\", \"qr.ae\", \"adf.ly\", \"bitly.com\", \"cur.lv\", \"tinyurl.com\", \"ity.im\", \"q.gs\",\n",
    "            \"po.st\", \"bc.vc\", \"twitthis.com\", \"u.to\", \"j.mp\", \"buzurl.com\", \"cutt.us\", \"u.bb\",\n",
    "            \"yourls.org\", \"prettylinkpro.com\", \"scrnch.me\", \"filoops.info\", \"vzturl.com\", \"qr.net\",\n",
    "            \"1url.com\", \"tweez.me\", \"v.gd\", \"link.zip.net\"]\n",
    "        match = r\"^(%s)\" % \"|\".join(map(re.escape, shorteners))\n",
    "        shortening_filter = full_df['url'].str.contains(match) # a filter of any url that matches a shortening services\n",
    "        logging.info(\"Made Filter\")\n",
    "        top_df = full_df[~shortening_filter] # this is done so that the top_df doesn't have any urls that are shortening services\n",
    "        shortening_df = full_df[shortening_filter]  # new df of only rows included with shortening services.\n",
    "        logging.info(\"Used Filters\")\n",
    "\n",
    "        print(full_df[full_df['murl_score'].isnull()])\n",
    "        full_df.dropna(subset=['murl_score'], inplace=True)\n",
    "        print(full_df[full_df['total_phish_score'].isnull()])\n",
    "        full_df.dropna(subset=['total_phish_score'], inplace=True)\n",
    "        print(full_df[full_df['total_score'].isnull()])\n",
    "        full_df.dropna(subset=['total_score'], inplace=True)\n",
    "\n",
    "        top_df = pd.concat([\n",
    "            full_df.loc[full_df.groupby('domain')['murl_score'].idxmax()],\n",
    "            full_df.loc[full_df.groupby('domain')['total_phish_score'].idxmax()],\n",
    "            full_df.loc[full_df.groupby('domain')['total_score'].idxmax()],\n",
    "            ])\n",
    "        logging.info(\"filtered by domain\")\n",
    "        top_df.drop_duplicates(subset='domain', inplace=True)  # leaves only 1 url per domain. \n",
    "        #^ in the future maybe give a reason to which is chosen? think currently it's whichever is first\n",
    "        logging.info(\"dropped duplicate domains\")\n",
    "        logging.info(f\"{len(top_df)} urls left\")\n",
    "\n",
    "        virus_total_df = high_scores_getter(top_df, 3500)\n",
    "        high_scores_df = high_scores_getter(top_df, 75)\n",
    "        \n",
    "        \n",
    "        virus_total_df['is_high_score'] = virus_total_df['url'].isin(high_scores_df['url'])\n",
    "        virus_total_df['has_been_virus_totaled'] = False\n",
    "        virus_total_df['positives'] = None\n",
    "        virus_total_df['total'] = None\n",
    "\n",
    "        dataframe_upload('TEST_shortening_services_results',previous_monday,shortening_df )\n",
    "        dataframe_upload('TEST_high_score_results',previous_monday,high_scores_df )\n",
    "        dataframe_upload('TEST_full_score_results',previous_monday,full_df )\n",
    "\n",
    "\n",
    "        \n",
    "        virus_schema=[\n",
    "            {\"name\": \"index\", \"type\": \"INTEGER\"},\n",
    "            {\"name\": \"url\", \"type\": \"STRING\"},\n",
    "            {\"name\": \"phish_score\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"murl_score\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"week_of\", \"type\": \"TIMESTAMP\"},\n",
    "            {\"name\": \"user_count_score\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"user_count\", \"type\": \"INTEGER\"},\n",
    "            {\"name\": \"users_list\", \"type\": \"STRING\"},\n",
    "            {\"name\": \"rdap_score\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"domain\", \"type\": \"STRING\"},\n",
    "            {\"name\": \"total_phish_score\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"total_score\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"tld_count\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"suspicious_keyword\", \"type\": \"INTEGER\"},\n",
    "            {\"name\": \"adult_sus_words\", \"type\": \"FLOAT\"},\n",
    "            {\"name\": \"is_high_score\", \"type\": \"BOOLEAN\"},\n",
    "            {\"name\": \"has_been_virus_totaled\", \"type\": \"BOOLEAN\"},\n",
    "            {\"name\": \"positives\", \"type\": \"INTEGER\"},\n",
    "            {\"name\": \"total\", \"type\": \"INTEGER\"}\n",
    "        ]\n",
    "        virus_total_df.to_gbq(destination_table=f'umrf_murl_v2_virustotal.virus_total_results_refined', \n",
    "            project_id='fxs-gccr-sbd-dev-sandbox', if_exists='append', table_schema=virus_schema)\n",
    "    \n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        logging.error(\"Outer function Failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_url_by_domain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855cc74-788e-4518-89f8-c08918b668ad",
   "metadata": {},
   "source": [
    "## <a id='pipeline'></a>\n",
    "# <center> Step #11: KubeFlow Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebdde4-3a33-4e46-9bc2-26b3b7a3fa50",
   "metadata": {},
   "source": [
    "### <center> Overwrite definitions with cached versions to speed up and save money during testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f94125-1268-4477-bf3d-d74adf3049e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THESE OUT AND CHANGE FILES WHENEVER YOU NEED TO USE CACHED VERSION FOR SPEED/MONEY\n",
    "\n",
    "def query_job():\n",
    "    return\n",
    "\n",
    "def early_RDAP():\n",
    "    return\n",
    "\n",
    "def megapreprocess_op():\n",
    "   return\n",
    "\n",
    "# def run_batch_preds():\n",
    "#     return\n",
    "\n",
    "def get_user_count_scoring():\n",
    "    return\n",
    "\n",
    "def combine_user_dfs():\n",
    "    return \n",
    "\n",
    "def rdap_scored_df():\n",
    "    return\n",
    "\n",
    "\n",
    "def top_url_by_domain():\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6872a-255c-4dd0-b471-b6929fad6232",
   "metadata": {},
   "source": [
    "## <a id='component'></a>\n",
    "# <center> Step #11.1: Component Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f9445-df13-4a24-9de1-91793396d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "query_job_component = comp.create_component_from_func(\n",
    "    func= query_job,  # Python function we're creating a component from\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['pandas','google-cloud-bigquery[pandas]', 'google-cloud-storage', 'pandas-gbq'],\n",
    "    output_component_file='query_job_comp.yaml'\n",
    ")\n",
    "\n",
    "query_custom_job = create_custom_training_job_from_component(\n",
    "    component_spec=query_job_component,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    display_name=\"Query custom job\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "preprocessing  = comp.create_component_from_func(\n",
    "    func = megapreprocess_op,\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['pandas', 'scikit-learn', 'tld', 'google-cloud-storage','fsspec','gcsfs', 'pandas-gbq','tldextract'],\n",
    "    output_component_file='preprocess_comp.yaml'\n",
    ")\n",
    "\n",
    "preprocessing_custom_job = create_custom_training_job_from_component(\n",
    "    preprocessing,\n",
    "    replica_count=1,\n",
    "    machine_type=\"c2-standard-30\",\n",
    "    display_name=\"preprocessing custom job\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "user_scoring = comp.create_component_from_func(\n",
    "    func =  get_user_count_scoring,\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['numpy','pandas','google-cloud-bigquery[pandas]', 'google-cloud-storage','fsspec','gcsfs', 'scikit-learn', 'pandas-gbq','tldextract'],\n",
    "    output_component_file='user_scoring_comp.yaml'\n",
    ")\n",
    "\n",
    "user_scoring_custom_job = create_custom_training_job_from_component(\n",
    "    user_scoring,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    display_name=\"User Scoring custom job\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "batch_predictions = comp.create_component_from_func(\n",
    "    func = run_batch_preds,\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['google-cloud-bigquery[pandas]', 'google-cloud-storage','google-cloud-aiplatform','fsspec','gcsfs', 'pandas-gbq','tldextract'],\n",
    "    output_component_file='run_batch_preds.yaml'\n",
    ")\n",
    "\n",
    "batch_predictions_custom_job = create_custom_training_job_from_component(\n",
    "    batch_predictions,\n",
    "    replica_count=1,\n",
    "    machine_type=\"c2-standard-30\",\n",
    "    display_name=\"Batch Predictions custom job\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "combine_dfs =  comp.create_component_from_func(\n",
    "    func =  combine_user_dfs,\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['numpy','pandas','google-cloud-bigquery[pandas]', 'google-cloud-storage','fsspec','gcsfs', 'pandas-gbq','tldextract'],\n",
    "    output_component_file='combine_dfs_comp.yaml'\n",
    ")\n",
    "\n",
    "combine_custom_job = create_custom_training_job_from_component(\n",
    "    combine_dfs,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-highmem-16\",\n",
    "    display_name=\"Combine outputs custom job\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "early_rdap_comp = comp.create_component_from_func(\n",
    "    func=early_RDAP,\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=['numpy', 'requests','pandas', 'google-cloud-storage','fsspec','gcsfs', 'google-cloud-aiplatform', 'pandas-gbq','tldextract'],\n",
    "    output_component_file='early_rdap_comp.yaml'\n",
    ")\n",
    "\n",
    "early_rdap_custom_job = create_custom_training_job_from_component(\n",
    "    early_rdap_comp,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-highmem-16\",\n",
    "    display_name=\"Early rdap custom job\",\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "rdap_scored_comp = comp.create_component_from_func(\n",
    "    func = rdap_scored_df,\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['numpy', 'requests','pandas', 'google-cloud-storage','fsspec','gcsfs', 'pandas-gbq', 'tldextract'],\n",
    "    output_component_file='rdap_comp.yaml'\n",
    ")\n",
    "\n",
    "new_total_scoring = comp.create_component_from_func(\n",
    "    func = top_url_by_domain,\n",
    "    base_image= 'python:3.9',\n",
    "    packages_to_install=['numpy', 'pandas', 'google-cloud-storage','fsspec','gcsfs', 'scikit-learn', 'pandas-gbq', 'tldextract'],\n",
    "    output_component_file='total_scoring_comp.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad9f5-457d-4a17-8a39-82939cdc182c",
   "metadata": {},
   "source": [
    "## <a id='define'></a>\n",
    "# <center> Step #11.2: Defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405a1f6-0ac3-47e9-89ce-ba46d8247590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import components as comp\n",
    "\n",
    "# TESTING\n",
    "# DEFINING PIPELINE\n",
    "@dsl.pipeline(\n",
    "    name='testrunpipeline',\n",
    "    description='ML pipeline test'\n",
    ")\n",
    "def pipeline2():\n",
    "    import logging\n",
    "    logging.info('Querying CIM Table Started')\n",
    "    # query_step = query_job_component()\n",
    "    query_step = query_custom_job(project=PROJECT_ID)\n",
    "\n",
    "    \n",
    "    logging.info('Preprocessing Started')\n",
    "    preprocessing_step = preprocessing_custom_job(project=PROJECT_ID).after(query_step)\n",
    "    logging.info('Preprocessing Done')\n",
    "    \n",
    "    logging.info('User Scoring Started')\n",
    "    user_scoring_step = user_scoring_custom_job(project=PROJECT_ID).after(query_step)\n",
    "    logging.info('User Scoring Done')\n",
    "    \n",
    "    logging.info('Early RDAP Started')\n",
    "    early_rdap_wait = early_rdap_custom_job(project=PROJECT_ID).after(query_step)\n",
    "    logging.info('Early RDAP Done')\n",
    "\n",
    "    logging.info('Batch Predictions started')\n",
    "    batch_predictions_step = batch_predictions_custom_job(project=PROJECT_ID).after(preprocessing_step)\n",
    "    logging.info('Batch Predictions Done')\n",
    "    \n",
    "    logging.info('Combining DFs Started')\n",
    "    combine_dfs_step = combine_custom_job(project=PROJECT_ID).after(batch_predictions_step, user_scoring_step, early_rdap_wait)\n",
    "    logging.info('Combining DFs Done')\n",
    "    \n",
    "    logging.info('Full RDAP Scoring Started')\n",
    "    rdap_scored_step = rdap_scored_comp().after(combine_dfs_step)\n",
    "    logging.info('RDAP Done')\n",
    "\n",
    "    logging.info('Total scoring started')\n",
    "    total_score_step = new_total_scoring().after(rdap_scored_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae6f7c-c574-4747-b35b-5ba85fe4ed8d",
   "metadata": {},
   "source": [
    "## <a id='compile'></a>\n",
    "# <center> Step 11.3: Compile The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d614f-a5ff-40bf-83c9-96fde1e72e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May get a deprecation warning message\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline2,\n",
    "    package_path=\"scaled_test.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2dbde-2a13-4066-bb97-dc7113bace9f",
   "metadata": {},
   "source": [
    "## <a id='submit'></a>\n",
    "# <center> Step 11.4: Submitting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef036d4e-b751-4ac9-b0ee-6fb9c5737227",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "pipeline_ = aiplatform.pipeline_jobs.PipelineJob(\n",
    "    enable_caching = False, # False\n",
    "    display_name = PIPELINE_NAME,\n",
    "    template_path = \"scaled_test.json\"\n",
    ")\n",
    "\n",
    "pipeline_.submit(service_account='my-bigquery-sa@fxs-gccr-sbd-dev-sandbox.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0ea65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
